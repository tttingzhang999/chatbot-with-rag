diff --git a/.env.example b/.env.example
index b667caa..442eb5c 100644
--- a/.env.example
+++ b/.env.example
@@ -16,6 +16,61 @@ DATABASE_URL=postgresql://postgres:password@localhost:5432/hr_chatbot
 APP_NAME=HR Chatbot
 DEBUG=true
 
+# ============================================
+# OPTIONAL - Server Configuration (has defaults)
+# ============================================
+
+# Backend API (FastAPI/Uvicorn)
+# UVICORN_HOST=0.0.0.0
+# UVICORN_PORT=8000
+# UVICORN_RELOAD=false
+
+# Frontend (Gradio)
+# GRADIO_HOST=0.0.0.0
+# GRADIO_PORT=7860
+
+# ============================================
+# OPTIONAL - API Configuration (has defaults)
+# ============================================
+
+# API Metadata
+# API_TITLE=HR Chatbot API
+# API_DESCRIPTION=API for HR Chatbot with RAG capabilities
+# API_VERSION=0.3.0
+
+# CORS Settings (comma-separated origins, or * for all)
+# CORS_ORIGINS=*
+
+# ============================================
+# OPTIONAL - File Upload Configuration (has defaults)
+# ============================================
+
+# UPLOAD_DIR=uploads
+# SUPPORTED_FILE_TYPES=pdf,txt,docx,doc
+
+# ============================================
+# OPTIONAL - Frontend Configuration (has defaults)
+# ============================================
+
+# BACKEND_API_URL=http://localhost:8000
+# ASSETS_DIR=assets
+# BOT_AVATAR_FILENAME=bot_avatar.png
+
+# ============================================
+# OPTIONAL - HTTP Configuration (has defaults)
+# ============================================
+
+# HTTP_TIMEOUT_DEFAULT=10
+# HTTP_TIMEOUT_UPLOAD=30
+# HTTP_TIMEOUT_SHORT=5
+
+# ============================================
+# OPTIONAL - Database Query Limits (has defaults)
+# ============================================
+
+# CONVERSATION_HISTORY_LIMIT=50
+# USER_CONVERSATIONS_LIMIT=20
+
 # ============================================
 # REQUIRED - AWS Configuration for Bedrock
 # ============================================
@@ -37,8 +92,8 @@ AWS_PROFILE=your-aws-profile
 # OPTIONAL - Bedrock Models (has defaults)
 # ============================================
 
-# LLM_MODEL_ID=anthropic.claude-sonnet-4
-# EMBEDDING_MODEL_ID=cohere.embed-v4
+# LLM_MODEL_ID=anthropic.claude-3-5-sonnet-20240620-v1:0
+# EMBEDDING_MODEL_ID=cohere.embed-v4:0
 
 # LLM Parameters
 # LLM_TEMPERATURE=0.7
@@ -58,4 +113,5 @@ AWS_PROFILE=your-aws-profile
 # CHUNK_OVERLAP=200
 # TOP_K_CHUNKS=10
 # SEMANTIC_SEARCH_RATIO=0.5
-# EMBEDDING_DIMENSION=1024
+# RELEVANCE_THRESHOLD=0.3
+# EMBEDDING_DIMENSION=1536
diff --git a/.gitignore b/.gitignore
index 8ff871d..03035d9 100644
--- a/.gitignore
+++ b/.gitignore
@@ -84,3 +84,6 @@ site/
 *.temp
 tmp/
 temp/
+
+# Documents
+*.pdf
diff --git a/alembic/versions/9ff57d084d8a_change_embedding_dimension_from_1024_to_.py b/alembic/versions/9ff57d084d8a_change_embedding_dimension_from_1024_to_.py
new file mode 100644
index 0000000..5276dee
--- /dev/null
+++ b/alembic/versions/9ff57d084d8a_change_embedding_dimension_from_1024_to_.py
@@ -0,0 +1,35 @@
+"""change embedding dimension from 1024 to 1536
+
+Revision ID: 9ff57d084d8a
+Revises: 34c68b28c841
+Create Date: 2025-11-26 15:08:36.275238
+
+"""
+from alembic import op
+import sqlalchemy as sa
+import pgvector.sqlalchemy
+
+
+# revision identifiers, used by Alembic.
+revision = '9ff57d084d8a'
+down_revision = '34c68b28c841'
+branch_labels = None
+depends_on = None
+
+
+def upgrade() -> None:
+    # ### commands auto generated by Alembic - please adjust! ###
+    op.alter_column('document_chunks', 'embedding',
+               existing_type=pgvector.sqlalchemy.vector.VECTOR(dim=1024),
+               type_=pgvector.sqlalchemy.vector.VECTOR(dim=1536),
+               existing_nullable=True)
+    # ### end Alembic commands ###
+
+
+def downgrade() -> None:
+    # ### commands auto generated by Alembic - please adjust! ###
+    op.alter_column('document_chunks', 'embedding',
+               existing_type=pgvector.sqlalchemy.vector.VECTOR(dim=1536),
+               type_=pgvector.sqlalchemy.vector.VECTOR(dim=1024),
+               existing_nullable=True)
+    # ### end Alembic commands ###
diff --git a/pyproject.toml b/pyproject.toml
index c221324..f6f958a 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -32,6 +32,10 @@ dependencies = [
     "langchain-community>=0.0.10",
     "langchain-aws>=0.1.0",
 
+    # Document Processing
+    "pypdf>=4.0.0",
+    "python-docx>=1.1.0",
+
     # Frontend
     "gradio>=4.0.0",
 
diff --git a/scripts/start_backend.sh b/scripts/start_backend.sh
index b1d28b2..198535b 100755
--- a/scripts/start_backend.sh
+++ b/scripts/start_backend.sh
@@ -1,17 +1,30 @@
 #!/bin/bash
 
-# Start FastAPI backend server
-
-echo "Starting FastAPI backend on http://localhost:8000"
-echo "API docs available at http://localhost:8000/docs"
-echo ""
+# Start FastAPI backend server (local development without AWS)
 
+# Change to project root directory
 cd "$(dirname "$0")/.." || exit
 
+# Load environment variables from .env file
+if [ -f ".env" ]; then
+    echo "Loading environment variables from .env file..."
+    set -a
+    source .env
+    set +a
+fi
+
+echo "=========================================="
+echo "Starting FastAPI backend (Local Mode)"
+echo "=========================================="
+echo "Backend URL: http://localhost:${UVICORN_PORT:-8000}"
+echo "API Docs:    http://localhost:${UVICORN_PORT:-8000}/docs"
+echo "=========================================="
+echo ""
+
 # Activate virtual environment if it exists
 if [ -d ".venv" ]; then
     source .venv/bin/activate
 fi
 
-# Start uvicorn server
-python -m uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
+# Start uvicorn server (config from settings)
+python -m uvicorn src.main:app --reload
diff --git a/scripts/start_backend_with_aws.sh b/scripts/start_backend_with_aws.sh
index aba4bef..14bc25b 100755
--- a/scripts/start_backend_with_aws.sh
+++ b/scripts/start_backend_with_aws.sh
@@ -3,9 +3,24 @@
 # Start FastAPI backend server with AWS Vault credentials
 # This script requires aws-vault to be configured
 
-# Check if AWS_PROFILE is set in .env
+# Change to project root directory
+cd "$(dirname "$0")/.." || exit
+
+# Load all environment variables from .env file
 if [ -f ".env" ]; then
-    export $(grep -v '^#' .env | grep AWS_PROFILE | xargs)
+    echo "Loading environment variables from .env file..."
+    # Export all non-comment, non-empty lines from .env
+    set -a
+    source .env
+    set +a
+else
+    echo "Warning: .env file not found!"
+fi
+
+# Check if AWS_REGION is set
+if [ -z "$AWS_REGION" ]; then
+    echo "Warning: AWS_REGION not set, using default: us-east-1"
+    export AWS_REGION=us-east-1
 fi
 
 # Use AWS_PROFILE from .env or ask user
@@ -15,18 +30,22 @@ if [ -z "$AWS_PROFILE" ]; then
     read AWS_PROFILE
 fi
 
-echo "Starting FastAPI backend with AWS Vault profile: $AWS_PROFILE"
-echo "Backend will be available at http://localhost:8000"
-echo "API docs at http://localhost:8000/docs"
+echo "=========================================="
+echo "Starting FastAPI backend with AWS Vault"
+echo "=========================================="
+echo "AWS Profile: $AWS_PROFILE"
+echo "AWS Region:  $AWS_REGION"
+echo "Backend URL: http://localhost:${UVICORN_PORT:-8000}"
+echo "API Docs:    http://localhost:${UVICORN_PORT:-8000}/docs"
+echo "=========================================="
 echo ""
 
-cd "$(dirname "$0")/.." || exit
-
 # Activate virtual environment if it exists
 if [ -d ".venv" ]; then
     source .venv/bin/activate
 fi
 
 # Start uvicorn server with aws-vault
-# Use full path to python to avoid PATH issues with aws-vault
-aws-vault exec "$AWS_PROFILE" -- .venv/bin/python -m uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
+# AWS_REGION must be exported for aws-vault to work
+export AWS_REGION
+aws-vault exec "$AWS_PROFILE" -- .venv/bin/python -m uvicorn src.main:app --reload
diff --git a/scripts/start_frontend.sh b/scripts/start_frontend.sh
index 132c2e3..543ccc5 100755
--- a/scripts/start_frontend.sh
+++ b/scripts/start_frontend.sh
@@ -2,12 +2,27 @@
 
 # Start Gradio frontend
 
-echo "Starting Gradio frontend on http://localhost:7860"
-echo "Make sure FastAPI backend is running on http://localhost:8000"
-echo ""
-
+# Change to project root directory
 cd "$(dirname "$0")/.." || exit
 
+# Load environment variables from .env file
+if [ -f ".env" ]; then
+    echo "Loading environment variables from .env file..."
+    set -a
+    source .env
+    set +a
+fi
+
+echo "=========================================="
+echo "Starting Gradio Frontend"
+echo "=========================================="
+echo "Frontend URL: http://localhost:${GRADIO_PORT:-7860}"
+echo "Backend API:  ${BACKEND_API_URL:-http://localhost:8000}"
+echo "=========================================="
+echo ""
+echo "âš ï¸  Make sure FastAPI backend is running!"
+echo ""
+
 # Activate virtual environment if it exists
 if [ -d ".venv" ]; then
     source .venv/bin/activate
diff --git a/scripts/test_api.py b/scripts/test_api.py
index dde4419..5051324 100644
--- a/scripts/test_api.py
+++ b/scripts/test_api.py
@@ -2,9 +2,12 @@
 Simple script to test the API endpoints.
 """
 
+import os
+
 import requests
 
-API_BASE_URL = "http://localhost:8000"
+# Read from environment variable or use default
+API_BASE_URL = os.getenv("BACKEND_API_URL", "http://localhost:8000")
 
 
 def test_health_check():
diff --git a/scripts/test_basic_processing.py b/scripts/test_basic_processing.py
new file mode 100644
index 0000000..fd74cdf
--- /dev/null
+++ b/scripts/test_basic_processing.py
@@ -0,0 +1,133 @@
+"""
+Basic test script for document processing (without AWS Bedrock).
+
+Tests only the local components:
+1. Text extraction from TXT/PDF/DOCX
+2. Text chunking with overlap
+3. Database operations
+"""
+
+import logging
+import sys
+import tempfile
+from pathlib import Path
+from uuid import uuid4
+
+# Add project root to path
+sys.path.insert(0, str(Path(__file__).parent.parent))
+
+from sqlalchemy import create_engine
+from sqlalchemy.orm import sessionmaker
+
+from src.core.config import settings
+from src.models import User
+from src.models.document import Document
+from src.services.document_service import DocumentProcessor
+
+# Configure logging
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+)
+logger = logging.getLogger(__name__)
+
+
+def create_test_document() -> str:
+    """Create a temporary test document."""
+    content = """
+    Employee Leave Policy
+
+    1. Annual Leave
+    All full-time employees are entitled to 15 days of annual leave per year.
+    Leave must be requested at least 2 weeks in advance for approval.
+
+    2. Sick Leave
+    Employees receive 10 days of paid sick leave annually.
+    A medical certificate is required for absences exceeding 2 consecutive days.
+
+    3. Remote Work Policy
+    Employees may work remotely up to 2 days per week.
+    Remote work requests must be approved by the direct manager.
+    """
+
+    temp_file = tempfile.NamedTemporaryFile(
+        mode="w", suffix=".txt", delete=False, encoding="utf-8"
+    )
+    temp_file.write(content)
+    temp_file.close()
+
+    return temp_file.name
+
+
+def test_text_extraction_and_chunking():
+    """Test text extraction and chunking without Bedrock."""
+    logger.info("\n" + "=" * 60)
+    logger.info("TEST: Text Extraction and Chunking (Local Only)")
+    logger.info("=" * 60)
+
+    # Create database session
+    engine = create_engine(settings.DATABASE_URL)
+    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
+    db = SessionLocal()
+
+    try:
+        # Get or create test user
+        test_user = db.query(User).filter(User.email == "test@example.com").first()
+        if not test_user:
+            test_user = User(
+                username="testuser",
+                email="test@example.com",
+                hashed_password="dummy_hash",
+                full_name="Test User",
+                is_active=True,
+                is_superuser=False,
+            )
+            db.add(test_user)
+            db.commit()
+            db.refresh(test_user)
+
+        logger.info(f"âœ“ Using test user: {test_user.id}")
+
+        # Create test document
+        test_file = create_test_document()
+        logger.info(f"âœ“ Created test file: {test_file}")
+
+        # Create processor
+        processor = DocumentProcessor(db)
+
+        # Test text extraction
+        logger.info("\n1. Testing text extraction...")
+        text = processor.extract_text(test_file, "txt")
+        logger.info(f"âœ“ Extracted {len(text)} characters")
+        logger.info(f"   Preview: {text[:100]}...")
+
+        # Test chunking
+        logger.info("\n2. Testing text chunking...")
+        chunks = processor.chunk_text(text)
+        logger.info(f"âœ“ Created {len(chunks)} chunks")
+        for idx, chunk in enumerate(chunks[:3], 1):
+            logger.info(f"   Chunk {idx}: {len(chunk)} chars - {chunk[:50]}...")
+
+        # Test BM25 indexing
+        logger.info("\n3. Testing BM25 index creation...")
+        bm25_vectors = processor.create_bm25_indexes(chunks)
+        logger.info(f"âœ“ Created {len(bm25_vectors)} BM25 vectors")
+
+        logger.info("\nâœ… All basic processing tests PASSED!")
+        logger.info("\nðŸ“ Note: To test full RAG functionality with embeddings,")
+        logger.info("   you need to configure AWS Bedrock credentials:")
+        logger.info("   1. Run: aws-vault add ting-bedrock")
+        logger.info("   2. Enter your AWS credentials")
+        logger.info("   3. Run: aws-vault exec ting-bedrock -- python scripts/test_rag.py")
+
+    except Exception as e:
+        logger.error(f"âœ— Test FAILED: {e}")
+        import traceback
+
+        traceback.print_exc()
+
+    finally:
+        db.close()
+
+
+if __name__ == "__main__":
+    test_text_extraction_and_chunking()
diff --git a/scripts/test_rag.py b/scripts/test_rag.py
new file mode 100644
index 0000000..938673d
--- /dev/null
+++ b/scripts/test_rag.py
@@ -0,0 +1,320 @@
+"""
+Test script for RAG (Retrieval-Augmented Generation) workflow.
+
+This script tests:
+1. Document upload and processing
+2. Embedding generation via Bedrock
+3. Hybrid search (Semantic + BM25)
+4. End-to-end RAG conversation
+"""
+
+import logging
+import sys
+import tempfile
+from pathlib import Path
+from uuid import uuid4
+
+# Add project root to path
+sys.path.insert(0, str(Path(__file__).parent.parent))
+
+from sqlalchemy import create_engine
+from sqlalchemy.orm import sessionmaker
+
+from src.core.config import settings
+from src.models import User
+from src.models.document import Document
+from src.services.document_service import DocumentProcessor
+from src.services.retrieval_service import get_retrieval_service
+
+# Configure logging
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+)
+logger = logging.getLogger(__name__)
+
+
+def create_test_document() -> str:
+    """
+    Create a temporary test document with sample HR content.
+
+    Returns:
+        Path to the temporary file
+    """
+    content = """
+    Employee Leave Policy
+
+    1. Annual Leave
+    All full-time employees are entitled to 15 days of annual leave per year.
+    Leave must be requested at least 2 weeks in advance for approval.
+    Unused leave can be carried forward to the next year, up to a maximum of 5 days.
+
+    2. Sick Leave
+    Employees receive 10 days of paid sick leave annually.
+    A medical certificate is required for absences exceeding 2 consecutive days.
+    Sick leave does not carry forward to the next year.
+
+    3. Remote Work Policy
+    Employees may work remotely up to 2 days per week.
+    Remote work requests must be approved by the direct manager.
+    Employees must be available during core hours (10 AM - 4 PM).
+
+    4. Performance Reviews
+    Performance reviews are conducted bi-annually in June and December.
+    Reviews include self-assessment and manager evaluation.
+    Results determine annual salary adjustments and bonus eligibility.
+
+    5. Professional Development
+    The company provides a $2,000 annual budget for professional development.
+    Employees can use this for courses, certifications, or conferences.
+    All development activities must align with role requirements.
+    """
+
+    # Create temp file
+    temp_file = tempfile.NamedTemporaryFile(
+        mode="w", suffix=".txt", delete=False, encoding="utf-8"
+    )
+    temp_file.write(content)
+    temp_file.close()
+
+    logger.info(f"Created test document: {temp_file.name}")
+    return temp_file.name
+
+
+def test_document_processing(db, user_id: str):
+    """
+    Test document upload and processing pipeline.
+
+    Args:
+        db: Database session
+        user_id: Test user ID
+    """
+    logger.info("\n" + "=" * 60)
+    logger.info("TEST 1: Document Processing Pipeline")
+    logger.info("=" * 60)
+
+    # Create test document
+    test_file = create_test_document()
+
+    try:
+        # Create document record
+        document_id = uuid4()
+        document = Document(
+            id=document_id,
+            user_id=user_id,
+            file_name="test_hr_policy.txt",
+            file_path=test_file,
+            file_type="txt",
+            file_size=Path(test_file).stat().st_size,
+            status="pending",
+        )
+        db.add(document)
+        db.commit()
+
+        logger.info(f"âœ“ Document record created: {document_id}")
+
+        # Process document
+        processor = DocumentProcessor(db)
+        result = processor.process_document_sync(
+            document_id=document_id, file_path=test_file, file_type="txt"
+        )
+
+        logger.info(f"âœ“ Document processing result: {result}")
+
+        if result["status"] == "success":
+            logger.info(f"âœ“ Created {result['chunks_created']} chunks")
+            logger.info("âœ“ Document processing PASSED")
+            return document_id
+        else:
+            logger.error(f"âœ— Document processing FAILED: {result.get('error')}")
+            return None
+
+    except Exception as e:
+        logger.error(f"âœ— Document processing FAILED with exception: {e}")
+        return None
+
+
+def test_hybrid_search(db, user_id: str):
+    """
+    Test hybrid search (semantic + BM25).
+
+    Args:
+        db: Database session
+        user_id: Test user ID
+    """
+    logger.info("\n" + "=" * 60)
+    logger.info("TEST 2: Hybrid Search")
+    logger.info("=" * 60)
+
+    try:
+        retrieval_service = get_retrieval_service(db)
+
+        # Test query 1: Should match annual leave policy
+        query1 = "How many days of annual leave do I get?"
+        logger.info(f"\nQuery 1: '{query1}'")
+
+        results1 = retrieval_service.hybrid_search(query_text=query1, top_k=3, user_id=user_id)
+
+        logger.info(f"Found {len(results1)} results:")
+        for idx, result in enumerate(results1, 1):
+            logger.info(f"\n  Result {idx}:")
+            logger.info(f"    File: {result['file_name']}")
+            logger.info(f"    Score: {result['score']:.4f}")
+            logger.info(f"    Content preview: {result['content'][:100]}...")
+
+        # Test query 2: Should match remote work policy
+        query2 = "What is the remote work policy?"
+        logger.info(f"\nQuery 2: '{query2}'")
+
+        results2 = retrieval_service.hybrid_search(query_text=query2, top_k=3, user_id=user_id)
+
+        logger.info(f"Found {len(results2)} results:")
+        for idx, result in enumerate(results2, 1):
+            logger.info(f"\n  Result {idx}:")
+            logger.info(f"    File: {result['file_name']}")
+            logger.info(f"    Score: {result['score']:.4f}")
+            logger.info(f"    Content preview: {result['content'][:100]}...")
+
+        if results1 and results2:
+            logger.info("\nâœ“ Hybrid search PASSED")
+            return True
+        else:
+            logger.error("\nâœ— Hybrid search FAILED: No results found")
+            return False
+
+    except Exception as e:
+        logger.error(f"\nâœ— Hybrid search FAILED with exception: {e}")
+        import traceback
+
+        traceback.print_exc()
+        return False
+
+
+def test_rag_conversation(db, user_id: str):
+    """
+    Test end-to-end RAG conversation.
+
+    Args:
+        db: Database session
+        user_id: Test user ID
+    """
+    logger.info("\n" + "=" * 60)
+    logger.info("TEST 3: End-to-End RAG Conversation")
+    logger.info("=" * 60)
+
+    try:
+        from src.services.chat_service import generate_response
+
+        # Test question about leave policy
+        question = "How many days of sick leave am I entitled to?"
+        logger.info(f"\nQuestion: '{question}'")
+
+        response, retrieved_chunks = generate_response(
+            user_message=question, conversation_history=[], db=db, user_id=user_id
+        )
+
+        logger.info(f"\nResponse:\n{response}")
+
+        if retrieved_chunks:
+            logger.info(f"\nRetrieved {len(retrieved_chunks)} chunks:")
+            for idx, chunk in enumerate(retrieved_chunks, 1):
+                logger.info(f"  {idx}. {chunk['file_name']} (score: {chunk['score']:.4f})")
+
+        if response and "10 days" in response.lower():
+            logger.info("\nâœ“ RAG conversation PASSED (correct answer detected)")
+            return True
+        elif response:
+            logger.warning(
+                "\nâš  RAG conversation completed but answer may be incorrect"
+                "\n  Expected mention of '10 days' of sick leave"
+            )
+            return True
+        else:
+            logger.error("\nâœ— RAG conversation FAILED: No response generated")
+            return False
+
+    except Exception as e:
+        logger.error(f"\nâœ— RAG conversation FAILED with exception: {e}")
+        import traceback
+
+        traceback.print_exc()
+        return False
+
+
+def main():
+    """Run all RAG tests."""
+    logger.info("Starting RAG Workflow Tests")
+    logger.info(f"Database: {settings.DATABASE_URL}")
+    logger.info(f"RAG Enabled: {settings.ENABLE_RAG}")
+
+    if not settings.ENABLE_RAG:
+        logger.error("RAG is not enabled in .env file! Set ENABLE_RAG=true")
+        sys.exit(1)
+
+    # Create database session
+    engine = create_engine(settings.DATABASE_URL)
+    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
+    db = SessionLocal()
+
+    try:
+        # Get or create test user
+        test_user = db.query(User).filter(User.email == "test@example.com").first()
+
+        if not test_user:
+            logger.info("Creating test user...")
+            test_user = User(
+                username="testuser",
+                email="test@example.com",
+                hashed_password="dummy_hash",  # Not used in tests
+                full_name="Test User",
+                is_active=True,
+                is_superuser=False,
+            )
+            db.add(test_user)
+            db.commit()
+            db.refresh(test_user)
+            logger.info(f"âœ“ Test user created: {test_user.id}")
+        else:
+            logger.info(f"âœ“ Using existing test user: {test_user.id}")
+
+        # Run tests
+        test_results = []
+
+        # Test 1: Document Processing
+        document_id = test_document_processing(db, test_user.id)
+        test_results.append(("Document Processing", document_id is not None))
+
+        if not document_id:
+            logger.error("Document processing failed, skipping remaining tests")
+            return
+
+        # Test 2: Hybrid Search
+        search_passed = test_hybrid_search(db, test_user.id)
+        test_results.append(("Hybrid Search", search_passed))
+
+        # Test 3: RAG Conversation
+        rag_passed = test_rag_conversation(db, test_user.id)
+        test_results.append(("RAG Conversation", rag_passed))
+
+        # Print summary
+        logger.info("\n" + "=" * 60)
+        logger.info("TEST SUMMARY")
+        logger.info("=" * 60)
+
+        all_passed = True
+        for test_name, passed in test_results:
+            status = "âœ“ PASSED" if passed else "âœ— FAILED"
+            logger.info(f"{test_name}: {status}")
+            if not passed:
+                all_passed = False
+
+        if all_passed:
+            logger.info("\nðŸŽ‰ All tests PASSED!")
+        else:
+            logger.info("\nâŒ Some tests FAILED")
+
+    finally:
+        db.close()
+
+
+if __name__ == "__main__":
+    main()
diff --git a/src/api/routes/chat.py b/src/api/routes/chat.py
index 2cd78c1..be413d1 100644
--- a/src/api/routes/chat.py
+++ b/src/api/routes/chat.py
@@ -91,15 +91,21 @@ def send_message(
         content=request.message,
     )
 
-    # Generate response (currently echo, will be replaced with LLM)
-    assistant_content = chat_service.generate_response(request.message, history)
+    # Generate response with RAG
+    assistant_content, retrieved_chunks = chat_service.generate_response(
+        user_message=request.message,
+        conversation_history=history,
+        db=db,
+        user_id=current_user.id,
+    )
 
-    # Save assistant message
+    # Save assistant message with retrieved chunks
     assistant_message = chat_service.save_message(
         db=db,
         conversation_id=conversation.id,
         role=MessageRole.ASSISTANT,
         content=assistant_content,
+        retrieved_chunks=retrieved_chunks,
     )
 
     return ChatResponse(
diff --git a/src/api/routes/upload.py b/src/api/routes/upload.py
index eae1657..e94b159 100644
--- a/src/api/routes/upload.py
+++ b/src/api/routes/upload.py
@@ -12,6 +12,7 @@ from fastapi import APIRouter, BackgroundTasks, File, HTTPException, UploadFile,
 from pydantic import BaseModel
 
 from src.api.deps import CurrentUser, DBSession
+from src.core.config import settings
 from src.models.document import Document
 from src.services.document_service import DocumentProcessor, delete_document, get_user_documents
 
@@ -20,7 +21,7 @@ logger = logging.getLogger(__name__)
 router = APIRouter(prefix="/upload", tags=["upload"])
 
 # Upload directory (for local development)
-UPLOAD_DIR = Path("uploads")
+UPLOAD_DIR = Path(settings.UPLOAD_DIR)
 UPLOAD_DIR.mkdir(exist_ok=True)
 
 
@@ -130,12 +131,16 @@ async def upload_document(
             detail="No filename provided",
         )
 
+    # Extract just the filename (without path) from potentially full path
+    original_filename = Path(file.filename).name
+
     # Get file extension
-    file_ext = Path(file.filename).suffix.lower().lstrip(".")
-    if file_ext not in ["pdf", "txt", "docx", "doc"]:
+    file_ext = Path(original_filename).suffix.lower().lstrip(".")
+    if file_ext not in settings.SUPPORTED_FILE_TYPES:
+        supported_types = ", ".join(settings.SUPPORTED_FILE_TYPES)
         raise HTTPException(
             status_code=status.HTTP_400_BAD_REQUEST,
-            detail=f"Unsupported file type: {file_ext}. Supported: pdf, txt, docx",
+            detail=f"Unsupported file type: {file_ext}. Supported: {supported_types}",
         )
 
     try:
@@ -145,7 +150,7 @@ async def upload_document(
 
         # Generate unique filename
         document_id = uuid.uuid4()
-        unique_filename = f"{document_id}_{file.filename}"
+        unique_filename = f"{document_id}_{original_filename}"
         file_path = UPLOAD_DIR / unique_filename
 
         # Save file to local storage
@@ -158,7 +163,7 @@ async def upload_document(
         document = Document(
             id=document_id,
             user_id=current_user.id,
-            file_name=file.filename,
+            file_name=original_filename,
             file_path=str(file_path),
             file_type=file_ext,
             file_size=file_size,
@@ -171,8 +176,6 @@ async def upload_document(
         logger.info(f"Document record created: {document_id}")
 
         # Trigger background processing (simulates Lambda)
-        from src.core.config import settings
-
         background_tasks.add_task(
             process_document_background,
             document_id=document_id,
@@ -183,7 +186,7 @@ async def upload_document(
 
         return UploadResponse(
             document_id=str(document_id),
-            filename=file.filename,
+            filename=original_filename,
             size=file_size,
             content_type=file.content_type or "application/octet-stream",
             message="File uploaded successfully. Processing started in background.",
diff --git a/src/app.py b/src/app.py
index 2778c16..a117bef 100644
--- a/src/app.py
+++ b/src/app.py
@@ -7,10 +7,12 @@ from pathlib import Path
 import gradio as gr
 import requests
 
+from src.core.config import settings
+
 # Paths / configuration
-API_BASE_URL = "http://localhost:8000"
-ASSETS_DIR = Path(__file__).resolve().parent.parent / "assets"
-BOT_AVATAR_PATH = ASSETS_DIR / "bot_avatar.png"
+API_BASE_URL = settings.BACKEND_API_URL
+ASSETS_DIR = Path(__file__).resolve().parent.parent / settings.ASSETS_DIR
+BOT_AVATAR_PATH = ASSETS_DIR / settings.BOT_AVATAR_FILENAME
 BOT_AVATAR_IMAGE = str(BOT_AVATAR_PATH) if BOT_AVATAR_PATH.exists() else None
 
 # Global state
@@ -73,7 +75,9 @@ def register(
 
         # Send registration request
         response = requests.post(
-            f"{API_BASE_URL}/auth/register", json=register_data, timeout=5
+            f"{API_BASE_URL}/auth/register",
+            json=register_data,
+            timeout=settings.HTTP_TIMEOUT_SHORT,
         )
 
         if response.status_code == 201:
@@ -124,7 +128,7 @@ def login(username: str, password: str) -> tuple[str, gr.update, gr.update, gr.u
         response = requests.post(
             f"{API_BASE_URL}/auth/login",
             json={"username": username.strip(), "password": password},
-            timeout=5,
+            timeout=settings.HTTP_TIMEOUT_SHORT,
         )
 
         if response.status_code == 200:
@@ -184,7 +188,7 @@ def send_message(
                 "conversation_id": current_conversation_id,
             },
             headers=get_auth_headers(),
-            timeout=10,
+            timeout=settings.HTTP_TIMEOUT_DEFAULT,
         )
 
         if response.status_code == 200:
@@ -229,7 +233,7 @@ def load_conversations() -> list:
         response = requests.get(
             f"{API_BASE_URL}/chat/conversations",
             headers=get_auth_headers(),
-            timeout=5,
+            timeout=settings.HTTP_TIMEOUT_SHORT,
         )
 
         if response.status_code == 200:
@@ -284,7 +288,7 @@ def load_conversation_messages(conversation_id: str) -> list:
         response = requests.get(
             f"{API_BASE_URL}/chat/conversations/{conversation_id}",
             headers=get_auth_headers(),
-            timeout=5,
+            timeout=settings.HTTP_TIMEOUT_SHORT,
         )
 
         if response.status_code == 200:
@@ -331,7 +335,7 @@ def upload_file(file) -> tuple[str, gr.update, list]:
                 f"{API_BASE_URL}/upload/document",
                 files=files,
                 headers=get_auth_headers(),
-                timeout=30,
+                timeout=settings.HTTP_TIMEOUT_UPLOAD,
             )
 
         if response.status_code == 200:
@@ -362,7 +366,7 @@ def load_documents() -> list:
         response = requests.get(
             f"{API_BASE_URL}/upload/documents",
             headers=get_auth_headers(),
-            timeout=5,
+            timeout=settings.HTTP_TIMEOUT_SHORT,
         )
 
         if response.status_code == 200:
@@ -408,7 +412,7 @@ def delete_document(document_id: str) -> tuple[str, gr.update, list]:
         response = requests.delete(
             f"{API_BASE_URL}/upload/documents/{document_id.strip()}",
             headers=get_auth_headers(),
-            timeout=5,
+            timeout=settings.HTTP_TIMEOUT_SHORT,
         )
 
         if response.status_code == 200:
@@ -582,7 +586,7 @@ with gr.Blocks(title="HR Chatbot", theme=gr.themes.Soft()) as demo:
                     gr.Markdown("### ðŸ“¤ ä¸Šå‚³æ–‡ä»¶")
                     file_upload = gr.File(
                         label="é¸æ“‡æª”æ¡ˆ",
-                        file_types=[".pdf", ".txt", ".docx"],
+                        file_types=[f".{ext}" for ext in settings.SUPPORTED_FILE_TYPES],
                     )
                     upload_btn = gr.Button("ä¸Šå‚³", variant="primary", interactive=False)
 
@@ -704,9 +708,9 @@ if __name__ == "__main__":
     print("=" * 60)
     print("ðŸš€ Starting HR Chatbot Gradio Frontend")
     print("=" * 60)
-    print("Frontend URL: http://localhost:7860")
-    print("Backend API:  http://localhost:8000")
-    print("API Docs:     http://localhost:8000/docs")
+    print(f"Frontend URL: http://localhost:{settings.GRADIO_PORT}")
+    print(f"Backend API:  {settings.BACKEND_API_URL}")
+    print(f"API Docs:     {settings.BACKEND_API_URL}/docs")
     print("=" * 60)
-    print("\nâš ï¸  Make sure FastAPI backend is running on port 8000\n")
-    demo.launch(server_name="0.0.0.0", server_port=7860)
+    print(f"\nâš ï¸  Make sure FastAPI backend is running on {settings.BACKEND_API_URL}\n")
+    demo.launch(server_name=settings.GRADIO_HOST, server_port=settings.GRADIO_PORT)
diff --git a/src/core/bedrock_client.py b/src/core/bedrock_client.py
index f696811..6230754 100644
--- a/src/core/bedrock_client.py
+++ b/src/core/bedrock_client.py
@@ -3,14 +3,18 @@ Amazon Bedrock client for LLM and embedding operations.
 
 This module provides a wrapper around AWS Bedrock services for:
 - Claude Sonnet 4 LLM invocations
+- Cohere Embed v4 embedding generation
 - Conversation history management
 - Error handling and retries
 """
 
+import json
 import logging
+import time
 from typing import Any
 
 import boto3
+from botocore.config import Config
 from langchain_aws import ChatBedrock
 from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
 
@@ -30,10 +34,27 @@ class BedrockClient:
         # so we only specify the region and let boto3 use the default credential chain
         self.session = boto3.Session(region_name=settings.AWS_REGION)
 
+        # Configure retry strategy with exponential backoff for throttling
+        retry_config = Config(
+            retries={
+                "max_attempts": 8,  # Increased from default 4
+                "mode": "adaptive",  # Adaptive mode adjusts retry behavior based on success/failure
+            },
+            # Add connection timeout and read timeout
+            connect_timeout=30,
+            read_timeout=60,
+        )
+
+        # Initialize bedrock-runtime client for embeddings with retry config
+        self.bedrock_runtime = self.session.client(
+            "bedrock-runtime",
+            config=retry_config,
+        )
+
         # Initialize ChatBedrock for Claude Sonnet 4
         self.llm = ChatBedrock(
             model_id=settings.LLM_MODEL_ID,
-            client=self.session.client("bedrock-runtime"),
+            client=self.bedrock_runtime,
             model_kwargs={
                 "temperature": settings.LLM_TEMPERATURE,
                 "top_p": settings.LLM_TOP_P,
@@ -42,8 +63,8 @@ class BedrockClient:
         )
 
         logger.info(
-            f"BedrockClient initialized with model: {settings.LLM_MODEL_ID}, "
-            f"region: {settings.AWS_REGION}"
+            f"BedrockClient initialized with LLM: {settings.LLM_MODEL_ID}, "
+            f"Embedding: {settings.EMBEDDING_MODEL_ID}, region: {settings.AWS_REGION}"
         )
 
     def invoke_claude(
@@ -97,6 +118,116 @@ class BedrockClient:
             logger.error(f"Failed to invoke Claude: {e}")
             raise
 
+    def generate_embeddings(
+        self,
+        texts: list[str],
+        input_type: str = "search_document",
+        batch_size: int = 96,
+    ) -> list[list[float]]:
+        """
+        Generate embeddings for a list of texts using Cohere Embed v4.
+
+        Args:
+            texts: List of text strings to embed
+            input_type: Type of input text. Options:
+                - "search_document": For document chunks (default)
+                - "search_query": For search queries
+            batch_size: Maximum number of texts to process in one API call (max 96)
+
+        Returns:
+            List of embedding vectors (1024 dimensions each)
+
+        Raises:
+            Exception: If the Bedrock API call fails
+        """
+        if not texts:
+            return []
+
+        # Cohere Embed v4 supports up to 96 texts per request
+        batch_size = min(batch_size, 96)
+        all_embeddings = []
+
+        try:
+            # Process in batches to avoid API limits
+            for i in range(0, len(texts), batch_size):
+                batch = texts[i : i + batch_size]
+                batch_num = i // batch_size + 1
+                total_batches = (len(texts) + batch_size - 1) // batch_size
+
+                logger.debug(
+                    f"Generating embeddings for batch {batch_num}/{total_batches} "
+                    f"({len(batch)} texts)"
+                )
+
+                # Add delay between batches to avoid rate limiting (except for first batch)
+                if i > 0:
+                    delay = 0.5  # 500ms delay between batches
+                    logger.debug(f"Adding {delay}s delay to avoid throttling")
+                    time.sleep(delay)
+
+                # Prepare request body for Cohere Embed v4
+                request_body = {
+                    "texts": batch,
+                    "input_type": input_type,
+                    "embedding_types": ["float"],
+                }
+
+                # Invoke Bedrock model with retry handling
+                try:
+                    response = self.bedrock_runtime.invoke_model(
+                        modelId=settings.EMBEDDING_MODEL_ID,
+                        contentType="application/json",
+                        accept="application/json",
+                        body=json.dumps(request_body),
+                    )
+                except Exception as e:
+                    # If throttling occurs even with retries, log and re-raise
+                    if "ThrottlingException" in str(e):
+                        logger.warning(
+                            f"Throttling detected on batch {batch_num}/{total_batches}, "
+                            "retries exhausted"
+                        )
+                    raise
+
+                # Parse response
+                response_body = json.loads(response["body"].read())
+
+                # Extract embeddings from response
+                # Cohere Embed v4 returns embeddings in 'embeddings' field
+                batch_embeddings = response_body.get("embeddings", {}).get("float", [])
+
+                if not batch_embeddings:
+                    raise ValueError("No embeddings returned from Bedrock API")
+
+                all_embeddings.extend(batch_embeddings)
+
+            logger.info(
+                f"Generated {len(all_embeddings)} embeddings "
+                f"({settings.EMBEDDING_DIMENSION} dimensions each)"
+            )
+
+            return all_embeddings
+
+        except Exception as e:
+            logger.error(f"Failed to generate embeddings: {e}")
+            raise
+
+    def generate_query_embedding(self, query: str) -> list[float]:
+        """
+        Generate embedding for a search query.
+
+        Args:
+            query: Search query text
+
+        Returns:
+            Embedding vector (1024 dimensions)
+
+        Raises:
+            Exception: If the Bedrock API call fails
+        """
+        embeddings = self.generate_embeddings([query], input_type="search_query")
+        return embeddings[0] if embeddings else []
+
     def _format_conversation_history(
         self, conversation_history: list[Message], system_prompt: str
     ) -> list[SystemMessage | HumanMessage | AIMessage]:
diff --git a/src/core/config.py b/src/core/config.py
index 4fca1b3..5605bae 100644
--- a/src/core/config.py
+++ b/src/core/config.py
@@ -1,12 +1,22 @@
 """
 Application configuration using Pydantic settings.
+
+All configuration values can be set via environment variables in .env file.
 """
 
+from pathlib import Path
+
+from pydantic import Field, field_validator
 from pydantic_settings import BaseSettings, SettingsConfigDict
 
 
 class Settings(BaseSettings):
-    """Application settings."""
+    """
+    Application settings with validation.
+
+    All settings can be overridden via environment variables.
+    See .env.example for documentation of each setting.
+    """
 
     model_config = SettingsConfigDict(
         env_file=".env",
@@ -15,45 +25,255 @@ class Settings(BaseSettings):
         extra="ignore",
     )
 
-    # Application
-    APP_NAME: str = "HR Chatbot"
-    DEBUG: bool = False
+    # ========== Application ==========
+    APP_NAME: str = Field(
+        default="HR Chatbot",
+        description="Application name displayed in UI and logs",
+    )
+    DEBUG: bool = Field(
+        default=False,
+        description="Enable debug mode for development",
+    )
 
-    # Security
-    SECRET_KEY: str  # Required: Secret key for JWT token signing
-    ALGORITHM: str = "HS256"  # JWT algorithm
-    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 7  # 7 days
+    # ========== Security ==========
+    SECRET_KEY: str = Field(
+        ...,
+        description="Required: Secret key for JWT token signing. Generate with: openssl rand -hex 32",
+    )
+    ALGORITHM: str = Field(
+        default="HS256",
+        description="JWT signing algorithm",
+    )
+    ACCESS_TOKEN_EXPIRE_MINUTES: int = Field(
+        default=60 * 24 * 7,
+        description="JWT token expiration time in minutes (default: 7 days)",
+    )
 
-    # AWS Configuration
-    AWS_REGION: str = "us-east-1"
-    AWS_PROFILE: str | None = None
+    # ========== Server Configuration ==========
+    UVICORN_HOST: str = Field(
+        default="0.0.0.0",
+        description="FastAPI/Uvicorn server host",
+    )
+    UVICORN_PORT: int = Field(
+        default=8000,
+        description="FastAPI/Uvicorn server port",
+        ge=1024,
+        le=65535,
+    )
+    UVICORN_RELOAD: bool = Field(
+        default=False,
+        description="Enable auto-reload for development (set to True in dev mode)",
+    )
+    GRADIO_HOST: str = Field(
+        default="0.0.0.0",
+        description="Gradio frontend server host",
+    )
+    GRADIO_PORT: int = Field(
+        default=7860,
+        description="Gradio frontend server port",
+        ge=1024,
+        le=65535,
+    )
 
-    # Database
-    DB_SECRET_NAME: str = ""  # Optional: Only needed for AWS Secrets Manager
-    DATABASE_URL: str  # Required: PostgreSQL connection string
+    # ========== API Configuration ==========
+    API_TITLE: str = Field(
+        default="HR Chatbot API",
+        description="API title shown in OpenAPI docs",
+    )
+    API_DESCRIPTION: str = Field(
+        default="API for HR Chatbot with RAG capabilities",
+        description="API description shown in OpenAPI docs",
+    )
+    API_VERSION: str = Field(
+        default="0.3.0",
+        description="API version number",
+    )
+    CORS_ORIGINS: list[str] = Field(
+        default=["*"],
+        description="Allowed CORS origins (use specific origins in production)",
+    )
+
+    # ========== AWS Configuration ==========
+    AWS_REGION: str = Field(
+        default="us-east-1",
+        description="AWS region for Bedrock and other services",
+    )
+    AWS_PROFILE: str | None = Field(
+        default=None,
+        description="AWS profile name (optional, for local development)",
+    )
+
+    # ========== Database ==========
+    DB_SECRET_NAME: str = Field(
+        default="",
+        description="Optional: AWS Secrets Manager secret name for database credentials",
+    )
+    DATABASE_URL: str = Field(
+        ...,
+        description="Required: PostgreSQL connection string (postgresql://user:password@host:port/dbname)",
+    )
+    CONVERSATION_HISTORY_LIMIT: int = Field(
+        default=50,
+        description="Maximum number of messages to retrieve for conversation history",
+        ge=1,
+    )
+    USER_CONVERSATIONS_LIMIT: int = Field(
+        default=20,
+        description="Maximum number of conversations to retrieve for a user",
+        ge=1,
+    )
+
+    # ========== S3 Configuration ==========
+    DOCUMENT_BUCKET: str = Field(
+        default="",
+        description="Optional: S3 bucket name for document storage (only needed when using S3)",
+    )
 
-    # S3
-    DOCUMENT_BUCKET: str = ""  # Optional: Only needed when using S3
+    # ========== File Upload Configuration ==========
+    UPLOAD_DIR: str = Field(
+        default="uploads",
+        description="Local directory for temporary file uploads",
+    )
+    SUPPORTED_FILE_TYPES: list[str] = Field(
+        default=["pdf", "txt", "docx", "doc"],
+        description="List of supported file extensions (without leading dot)",
+    )
+
+    # ========== Frontend Configuration ==========
+    BACKEND_API_URL: str = Field(
+        default="http://localhost:8000",
+        description="Backend API URL for frontend to connect to",
+    )
+    ASSETS_DIR: str = Field(
+        default="assets",
+        description="Directory containing frontend assets (avatars, images, etc.)",
+    )
+    BOT_AVATAR_FILENAME: str = Field(
+        default="bot_avatar.png",
+        description="Filename of bot avatar image in assets directory",
+    )
+
+    # ========== HTTP Configuration ==========
+    HTTP_TIMEOUT_DEFAULT: int = Field(
+        default=30,
+        description="Default HTTP request timeout in seconds",
+        ge=1,
+    )
+    HTTP_TIMEOUT_UPLOAD: int = Field(
+        default=30,
+        description="HTTP timeout for file upload requests in seconds",
+        ge=1,
+    )
+    HTTP_TIMEOUT_SHORT: int = Field(
+        default=30,
+        description="HTTP timeout for quick requests (status checks, etc.) in seconds",
+        ge=1,
+    )
+
+    # ========== Bedrock Models ==========
+    LLM_MODEL_ID: str = Field(
+        default="anthropic.claude-3-5-sonnet-20240620-v1:0",
+        description="Amazon Bedrock LLM model ID",
+    )
+    EMBEDDING_MODEL_ID: str = Field(
+        default="cohere.embed-v4:0",
+        description="Amazon Bedrock embedding model ID",
+    )
+
+    # ========== LLM Configuration ==========
+    LLM_TEMPERATURE: float = Field(
+        default=0.7,
+        description="LLM sampling temperature (0.0-1.0, higher = more creative)",
+        ge=0.0,
+        le=1.0,
+    )
+    LLM_TOP_P: float = Field(
+        default=0.9,
+        description="LLM nucleus sampling top-p value (0.0-1.0)",
+        ge=0.0,
+        le=1.0,
+    )
+    LLM_MAX_TOKENS: int = Field(
+        default=2048,
+        description="Maximum tokens in LLM response",
+        ge=1,
+    )
+    MAX_CONVERSATION_HISTORY: int = Field(
+        default=10,
+        description="Number of recent conversation turns to include in context",
+        ge=1,
+    )
+
+    # ========== RAG Configuration ==========
+    CHUNK_SIZE: int = Field(
+        default=1000,
+        description="Document chunk size in characters",
+        ge=100,
+    )
+    CHUNK_OVERLAP: int = Field(
+        default=200,
+        description="Overlap between consecutive chunks in characters",
+        ge=0,
+    )
+    TOP_K_CHUNKS: int = Field(
+        default=10,
+        description="Number of top chunks to retrieve for RAG",
+        ge=1,
+    )
+    SEMANTIC_SEARCH_RATIO: float = Field(
+        default=0.5,
+        description="Ratio of semantic search in hybrid search (0.5 = 50% semantic, 50% BM25)",
+        ge=0.0,
+        le=1.0,
+    )
+    RELEVANCE_THRESHOLD: float = Field(
+        default=0.3,
+        description="Minimum relevance score threshold for RAG retrieval (0.0-1.0, higher = stricter)",
+        ge=0.0,
+        le=1.0,
+    )
+    ENABLE_RAG: bool = Field(
+        default=False,
+        description="Toggle RAG functionality on/off",
+    )
+    EMBEDDING_DIMENSION: int = Field(
+        default=1536,
+        description="Embedding vector dimension (Cohere Embed v4 via Bedrock uses 1536)",
+        ge=1,
+    )
 
-    # Bedrock Models
-    LLM_MODEL_ID: str = "anthropic.claude-sonnet-4"
-    EMBEDDING_MODEL_ID: str = "cohere.embed-v4"
+    @field_validator("UVICORN_PORT", "GRADIO_PORT")
+    @classmethod
+    def validate_port(cls, v: int) -> int:
+        """Validate port number is in valid range."""
+        if not (1024 <= v <= 65535):
+            raise ValueError(f"Port must be between 1024 and 65535, got {v}")
+        return v
 
-    # LLM Configuration
-    LLM_TEMPERATURE: float = 0.7
-    LLM_TOP_P: float = 0.9
-    LLM_MAX_TOKENS: int = 2048
-    MAX_CONVERSATION_HISTORY: int = 10  # Number of recent conversation turns to include
+    @field_validator("CHUNK_OVERLAP")
+    @classmethod
+    def validate_chunk_overlap(cls, v: int, info) -> int:
+        """Validate chunk overlap is less than chunk size."""
+        # Note: chunk_size might not be set yet during validation
+        # We'll do runtime validation in the document service
+        if v < 0:
+            raise ValueError("Chunk overlap must be non-negative")
+        return v
 
-    # RAG Configuration
-    CHUNK_SIZE: int = 1000
-    CHUNK_OVERLAP: int = 200
-    TOP_K_CHUNKS: int = 10
-    SEMANTIC_SEARCH_RATIO: float = 0.5  # 50% semantic, 50% BM25
-    ENABLE_RAG: bool = False  # Toggle RAG functionality
+    @field_validator("SUPPORTED_FILE_TYPES")
+    @classmethod
+    def validate_file_types(cls, v: list[str]) -> list[str]:
+        """Ensure file types are lowercase and without leading dots."""
+        return [ext.lower().lstrip(".") for ext in v]
 
-    # Cohere Embed v4 dimensions
-    EMBEDDING_DIMENSION: int = 1024
+    @field_validator("CORS_ORIGINS", mode="before")
+    @classmethod
+    def validate_cors_origins(cls, v) -> list[str]:
+        """Parse CORS origins from string or list."""
+        if isinstance(v, str):
+            # Support comma-separated string
+            return [origin.strip() for origin in v.split(",")]
+        return v
 
 
 settings = Settings()
diff --git a/src/main.py b/src/main.py
index 0346685..c0a1215 100644
--- a/src/main.py
+++ b/src/main.py
@@ -6,18 +6,19 @@ from fastapi import FastAPI
 from fastapi.middleware.cors import CORSMiddleware
 
 from src.api.routes import auth, chat, upload
+from src.core.config import settings
 
 # Create FastAPI app
 app = FastAPI(
-    title="HR Chatbot API",
-    description="API for HR Chatbot with RAG capabilities",
-    version="0.0.1",
+    title=settings.API_TITLE,
+    description=settings.API_DESCRIPTION,
+    version=settings.API_VERSION,
 )
 
 # Configure CORS
 app.add_middleware(
     CORSMiddleware,
-    allow_origins=["*"],  # In production, specify exact origins
+    allow_origins=settings.CORS_ORIGINS,
     allow_credentials=True,
     allow_methods=["*"],
     allow_headers=["*"],
@@ -33,8 +34,8 @@ app.include_router(upload.router)
 def root() -> dict:
     """Root endpoint."""
     return {
-        "message": "HR Chatbot API",
-        "version": "0.0.1",
+        "message": settings.API_TITLE,
+        "version": settings.API_VERSION,
         "docs": "/docs",
     }
 
@@ -50,7 +51,7 @@ if __name__ == "__main__":
 
     uvicorn.run(
         "src.main:app",
-        host="0.0.0.0",
-        port=8000,
-        reload=True,
+        host=settings.UVICORN_HOST,
+        port=settings.UVICORN_PORT,
+        reload=settings.UVICORN_RELOAD,
     )
diff --git a/src/models/document.py b/src/models/document.py
index 40754c0..bd720d5 100644
--- a/src/models/document.py
+++ b/src/models/document.py
@@ -71,8 +71,8 @@ class DocumentChunk(Base):
     chunk_index = Column(Integer, nullable=False)  # Order within document
     content = Column(Text, nullable=False)  # Actual text content
 
-    # Vector embedding for semantic search (Cohere Embed v4: 1024 dimensions)
-    embedding = Column(Vector(1024))
+    # Vector embedding for semantic search (Cohere Embed v4 via Bedrock: 1536 dimensions)
+    embedding = Column(Vector(1536))
 
     # Full-text search vector for BM25/TFIDF search
     content_tsvector = Column(TSVECTOR)
diff --git a/src/prompts/system_prompts.py b/src/prompts/system_prompts.py
index 67b2b41..2ece1f0 100644
--- a/src/prompts/system_prompts.py
+++ b/src/prompts/system_prompts.py
@@ -72,11 +72,17 @@ The following relevant documents have been retrieved to help answer the user's q
 {context}
 
 **Important Instructions:**
-- Base your answer primarily on the provided document context
+- **CRITICAL**: First evaluate if the retrieved documents are truly relevant to the user's question
+- If the user is just greeting (e.g., "å“ˆå›‰", "ä½ å¥½", "hi", "hello"), respond naturally and warmly WITHOUT mentioning documents or policies
+- If the retrieved documents are about a completely different topic than the question, explicitly tell the user:
+  "æŠ±æ­‰ï¼Œæˆ‘ç›®å‰çš„æ–‡æª”åº«ä¸­æ²’æœ‰èˆ‡æ‚¨å•é¡Œç›´æŽ¥ç›¸é—œçš„è³‡æ–™ã€‚ä¸éŽæˆ‘é‚„æ˜¯å¾ˆæ¨‚æ„å”åŠ©æ‚¨ï¼è«‹å•æ‚¨æœ‰ä»€éº¼HRç›¸é—œçš„å•é¡Œå—Žï¼Ÿ"
+- Only use document context when it's clearly and directly relevant to the user's question
+- Base your answer primarily on the provided document context when relevant
 - If the context doesn't contain sufficient information, acknowledge this clearly
 - Cite specific sections or documents when referencing policies
 - If context contradicts general HR knowledge, prioritize the company-specific context
 - Do not make up information not present in the context
+- Do not force-fit irrelevant document content into your answer
 
 **Response Format:**
 - Start with a direct answer to the question
diff --git a/src/services/chat_service.py b/src/services/chat_service.py
index d5a63f7..33a7939 100644
--- a/src/services/chat_service.py
+++ b/src/services/chat_service.py
@@ -89,16 +89,23 @@ def save_message(
     return message
 
 
-def generate_response(user_message: str, conversation_history: list[Message]) -> str:
+def generate_response(
+    user_message: str,
+    conversation_history: list[Message],
+    db: Session | None = None,
+    user_id: uuid.UUID | None = None,
+) -> tuple[str, list[dict] | None]:
     """
     Generate response to user message using Claude Sonnet 4 via Bedrock.
 
     Args:
         user_message: User's message
         conversation_history: Previous messages in the conversation
+        db: Database session (required if RAG is enabled)
+        user_id: User ID (required if RAG is enabled)
 
     Returns:
-        str: Generated response from Claude
+        tuple: (Generated response, Retrieved chunks info or None)
 
     Raises:
         Exception: If the LLM call fails
@@ -113,8 +120,66 @@ def generate_response(user_message: str, conversation_history: list[Message]) ->
             f"(limit: {settings.MAX_CONVERSATION_HISTORY} turns)"
         )
 
-        # Get system prompt (RAG disabled for now)
-        system_prompt = get_system_prompt(use_rag=settings.ENABLE_RAG)
+        # Initialize retrieved chunks
+        retrieved_chunks = None
+        retrieval_context = ""
+
+        # Perform RAG retrieval if enabled
+        if settings.ENABLE_RAG:
+            if not db or not user_id:
+                logger.warning("RAG enabled but db/user_id not provided, skipping retrieval")
+            else:
+                try:
+                    from src.services.retrieval_service import get_retrieval_service
+
+                    logger.info("Performing hybrid search for RAG context")
+
+                    # Get retrieval service and perform hybrid search
+                    retrieval_service = get_retrieval_service(db)
+                    search_results = retrieval_service.hybrid_search(
+                        query_text=user_message,
+                        top_k=settings.TOP_K_CHUNKS,
+                        user_id=user_id,
+                    )
+
+                    if search_results:
+                        # Format retrieved chunks for context
+                        context_parts = []
+                        retrieved_chunks = []
+
+                        for idx, result in enumerate(search_results, 1):
+                            context_parts.append(
+                                f"[Document {idx}: {result['file_name']}]\n"
+                                f"{result['content']}\n"
+                            )
+
+                            retrieved_chunks.append(
+                                {
+                                    "chunk_id": result["chunk_id"],
+                                    "document_id": result["document_id"],
+                                    "file_name": result["file_name"],
+                                    "score": result["score"],
+                                    "semantic_score": result.get("semantic_score"),
+                                    "bm25_score": result.get("bm25_score"),
+                                }
+                            )
+
+                        retrieval_context = "\n".join(context_parts)
+                        logger.info(
+                            f"Retrieved {len(search_results)} chunks for RAG context "
+                            f"(total {len(retrieval_context)} characters)"
+                        )
+                    else:
+                        logger.info("No relevant documents found for user query")
+
+                except Exception as e:
+                    logger.error(f"RAG retrieval failed: {e}")
+                    # Continue without RAG context rather than failing completely
+
+        # Get system prompt with or without RAG context
+        system_prompt = get_system_prompt(
+            use_rag=settings.ENABLE_RAG, context=retrieval_context
+        )
 
         # Get Bedrock client and invoke Claude
         bedrock_client = get_bedrock_client()
@@ -125,21 +190,33 @@ def generate_response(user_message: str, conversation_history: list[Message]) ->
         )
 
         logger.info("Response generated successfully")
-        return response
+        return response, retrieved_chunks
 
     except Exception as e:
         logger.error(f"Failed to generate response: {e}")
-        # Return a user-friendly error message
-        return (
-            "Sorry, I encountered an issue processing your request. "
-            "Please try again later or contact technical support."
-        )
+
+        # Check if this is a throttling error
+        error_str = str(e)
+        if "ThrottlingException" in error_str or "Too many requests" in error_str:
+            error_msg = (
+                "I'm currently experiencing high traffic and need to slow down. "
+                "Please wait a few seconds and try again. "
+                "If this persists, please wait 1-2 minutes before retrying."
+            )
+        else:
+            # Return a generic user-friendly error message
+            error_msg = (
+                "Sorry, I encountered an issue processing your request. "
+                "Please try again later or contact technical support."
+            )
+
+        return error_msg, None
 
 
 def get_conversation_history(
     db: Session,
     conversation_id: uuid.UUID,
-    limit: int = 50,
+    limit: int | None = None,
 ) -> list[Message]:
     """
     Get conversation history.
@@ -147,11 +224,14 @@ def get_conversation_history(
     Args:
         db: Database session
         conversation_id: Conversation ID
-        limit: Maximum number of messages to retrieve
+        limit: Maximum number of messages to retrieve (defaults to config setting)
 
     Returns:
         list[Message]: List of messages
     """
+    if limit is None:
+        limit = settings.CONVERSATION_HISTORY_LIMIT
+
     return (
         db.query(Message)
         .filter(Message.conversation_id == conversation_id)
@@ -164,7 +244,7 @@ def get_conversation_history(
 def get_user_conversations(
     db: Session,
     user_id: uuid.UUID,
-    limit: int = 20,
+    limit: int | None = None,
 ) -> list[tuple[Conversation, int]]:
     """
     Get all conversations for a user with message counts.
@@ -172,11 +252,14 @@ def get_user_conversations(
     Args:
         db: Database session
         user_id: User ID
-        limit: Maximum number of conversations to retrieve
+        limit: Maximum number of conversations to retrieve (defaults to config setting)
 
     Returns:
         list[tuple[Conversation, int]]: List of (conversation, message_count) tuples
     """
+    if limit is None:
+        limit = settings.USER_CONVERSATIONS_LIMIT
+
     # Use a subquery to count messages for each conversation efficiently
     message_count_subquery = (
         db.query(
diff --git a/src/services/document_service.py b/src/services/document_service.py
index 2bebf25..721afd6 100644
--- a/src/services/document_service.py
+++ b/src/services/document_service.py
@@ -15,6 +15,7 @@ from uuid import UUID
 from sqlalchemy import delete, func, select, update
 from sqlalchemy.orm import Session
 
+from src.core.config import settings
 from src.models.document import Document, DocumentChunk
 
 logger = logging.getLogger(__name__)
@@ -30,8 +31,8 @@ class DocumentProcessor:
     def __init__(self, db: Session):
         """Initialize processor with database session."""
         self.db = db
-        self.chunk_size = 512  # Default chunk size in characters
-        self.chunk_overlap = 128  # Default overlap size
+        self.chunk_size = settings.CHUNK_SIZE
+        self.chunk_overlap = settings.CHUNK_OVERLAP
 
     def process_document_sync(
         self,
@@ -113,44 +114,75 @@ class DocumentProcessor:
         Returns:
             Extracted text content
 
-        Note:
-            This is a placeholder implementation. For production:
-            - PDF: Use pypdf, PyMuPDF, or pdfplumber
-            - DOCX: Use python-docx
-            - TXT: Direct read with encoding detection
+        Raises:
+            FileNotFoundError: If file doesn't exist
+            ValueError: If file type is unsupported
         """
         if not os.path.exists(file_path):
             raise FileNotFoundError(f"File not found: {file_path}")
 
         file_type = file_type.lower()
 
-        if file_type == "txt":
-            # Read text file directly
-            with open(file_path, encoding="utf-8", errors="ignore") as f:
-                return f.read()
-
-        elif file_type == "pdf":
-            # TODO: Implement PDF extraction
-            # Recommended: pypdf or PyMuPDF (fitz)
-            # Example:
-            # from pypdf import PdfReader
-            # reader = PdfReader(file_path)
-            # text = "\n".join([page.extract_text() for page in reader.pages])
-            logger.warning("PDF extraction not yet implemented, returning placeholder")
-            return f"[PDF content placeholder from {Path(file_path).name}]\n\nThis is sample text content that would be extracted from the PDF file. In production, this will be replaced with actual text extraction using pypdf or PyMuPDF library."
-
-        elif file_type in ["docx", "doc"]:
-            # TODO: Implement DOCX extraction
-            # Recommended: python-docx
-            # Example:
-            # from docx import Document
-            # doc = Document(file_path)
-            # text = "\n".join([para.text for para in doc.paragraphs])
-            logger.warning("DOCX extraction not yet implemented, returning placeholder")
-            return f"[DOCX content placeholder from {Path(file_path).name}]\n\nThis is sample text content that would be extracted from the DOCX file. In production, this will be replaced with actual text extraction using python-docx library."
-
-        else:
-            raise ValueError(f"Unsupported file type: {file_type}")
+        try:
+            if file_type == "txt":
+                # Read text file with UTF-8 encoding
+                with open(file_path, encoding="utf-8", errors="ignore") as f:
+                    text = f.read()
+                logger.info(f"Extracted {len(text)} characters from TXT file")
+                return text
+
+            elif file_type == "pdf":
+                # Extract text from PDF using pypdf
+                from pypdf import PdfReader
+
+                reader = PdfReader(file_path)
+                text_parts = []
+
+                for page_num, page in enumerate(reader.pages):
+                    try:
+                        page_text = page.extract_text()
+                        if page_text.strip():
+                            text_parts.append(page_text)
+                    except Exception as e:
+                        logger.warning(f"Failed to extract text from page {page_num}: {e}")
+                        continue
+
+                text = "\n\n".join(text_parts)
+                logger.info(f"Extracted {len(text)} characters from {len(reader.pages)} PDF pages")
+                return text
+
+            elif file_type in ["docx", "doc"]:
+                # Extract text from DOCX using python-docx
+                from docx import Document
+
+                doc = Document(file_path)
+                text_parts = []
+
+                # Extract text from paragraphs
+                for para in doc.paragraphs:
+                    if para.text.strip():
+                        text_parts.append(para.text)
+
+                # Extract text from tables
+                for table in doc.tables:
+                    for row in table.rows:
+                        for cell in row.cells:
+                            if cell.text.strip():
+                                text_parts.append(cell.text)
+
+                text = "\n".join(text_parts)
+                logger.info(f"Extracted {len(text)} characters from DOCX file")
+                return text
+
+            else:
+                raise ValueError(f"Unsupported file type: {file_type}")
+
+        except ImportError as e:
+            logger.error(f"Missing dependency for {file_type} extraction: {e}")
+            raise ValueError(
+                f"Cannot extract {file_type} files. Missing required library. "
+                f"Please install dependencies."
+            )
 
     def chunk_text(
         self,
@@ -217,7 +249,7 @@ class DocumentProcessor:
 
     def generate_embeddings(self, chunks: list[str]) -> list[list[float]]:
         """
-        Generate embeddings for text chunks using Cohere Embed v4.
+        Generate embeddings for text chunks using Cohere Embed v4 via Bedrock.
 
         Args:
             chunks: List of text chunks
@@ -225,29 +257,23 @@ class DocumentProcessor:
         Returns:
             List of embedding vectors (1024 dimensions each)
 
-        Note:
-            This is a placeholder. For production, use:
-            - Amazon Bedrock with Cohere Embed v4
-            - Model ID: cohere.embed-english-v3 or cohere.embed-multilingual-v3
+        Raises:
+            Exception: If embedding generation fails
         """
-        # TODO: Implement Bedrock embedding generation
-        # Example using boto3:
-        # import boto3
-        # bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')
-        # response = bedrock.invoke_model(
-        #     modelId='cohere.embed-english-v3',
-        #     body=json.dumps({
-        #         'texts': chunks,
-        #         'input_type': 'search_document',
-        #         'embedding_types': ['float']
-        #     })
-        # )
-
-        logger.warning("Bedrock embedding generation not yet implemented, using mock")
-
-        # Return mock embeddings (1024 dimensions, all zeros)
-        # In production, this will return actual embeddings from Cohere
-        return [[0.0] * 1024 for _ in chunks]
+        from src.core.bedrock_client import get_bedrock_client
+
+        try:
+            bedrock_client = get_bedrock_client()
+            embeddings = bedrock_client.generate_embeddings(
+                texts=chunks, input_type="search_document"
+            )
+
+            logger.info(f"Generated {len(embeddings)} embeddings using Bedrock")
+            return embeddings
+
+        except Exception as e:
+            logger.error(f"Failed to generate embeddings via Bedrock: {e}")
+            raise
 
     def create_bm25_indexes(self, chunks: list[str]) -> list[str]:
         """
diff --git a/src/services/retrieval_service.py b/src/services/retrieval_service.py
new file mode 100644
index 0000000..6958bed
--- /dev/null
+++ b/src/services/retrieval_service.py
@@ -0,0 +1,360 @@
+"""
+Retrieval service for hybrid search (Semantic + BM25).
+
+This module implements:
+- Semantic search using pgvector cosine similarity
+- BM25 full-text search using PostgreSQL ts_rank
+- Hybrid search with score normalization and weighted merging
+"""
+
+import logging
+from typing import Any
+from uuid import UUID
+
+from sqlalchemy import func, text
+from sqlalchemy.orm import Session
+
+from src.core.bedrock_client import get_bedrock_client
+from src.core.config import settings
+from src.models.document import Document, DocumentChunk
+
+logger = logging.getLogger(__name__)
+
+
+class RetrievalService:
+    """Service for retrieving relevant document chunks using hybrid search."""
+
+    def __init__(self, db: Session):
+        """
+        Initialize retrieval service.
+
+        Args:
+            db: Database session
+        """
+        self.db = db
+        self.bedrock_client = get_bedrock_client()
+
+    def semantic_search(
+        self,
+        query_embedding: list[float],
+        top_k: int,
+        user_id: UUID | None = None,
+    ) -> list[dict[str, Any]]:
+        """
+        Perform semantic search using vector similarity.
+
+        Args:
+            query_embedding: Query embedding vector (1024 dimensions)
+            top_k: Number of results to return
+            user_id: Optional user ID to filter documents by owner
+
+        Returns:
+            List of dicts with chunk info and cosine similarity scores
+        """
+        try:
+            # Build query with vector similarity
+            # pgvector uses <=> for cosine distance (lower is better)
+            # We convert to similarity: 1 - distance
+            query = (
+                self.db.query(
+                    DocumentChunk.id,
+                    DocumentChunk.content,
+                    DocumentChunk.chunk_metadata,
+                    DocumentChunk.document_id,
+                    Document.file_name,
+                    # Cosine similarity = 1 - cosine distance
+                    (1 - DocumentChunk.embedding.cosine_distance(query_embedding)).label(
+                        "similarity"
+                    ),
+                )
+                .join(Document, DocumentChunk.document_id == Document.id)
+                .filter(Document.status == "completed")
+            )
+
+            # Filter by user if specified
+            if user_id:
+                query = query.filter(Document.user_id == user_id)
+
+            # Order by similarity (highest first) and limit
+            results = query.order_by(text("similarity DESC")).limit(top_k).all()
+
+            chunks = []
+            for row in results:
+                chunks.append(
+                    {
+                        "chunk_id": str(row.id),
+                        "content": row.content,
+                        "metadata": row.chunk_metadata or {},
+                        "document_id": str(row.document_id),
+                        "file_name": row.file_name,
+                        "score": float(row.similarity),
+                        "search_type": "semantic",
+                    }
+                )
+
+            logger.info(f"Semantic search returned {len(chunks)} results")
+            return chunks
+
+        except Exception as e:
+            logger.error(f"Semantic search failed: {e}")
+            raise
+
+    def bm25_search(
+        self,
+        query_text: str,
+        top_k: int,
+        user_id: UUID | None = None,
+    ) -> list[dict[str, Any]]:
+        """
+        Perform BM25 full-text search using PostgreSQL ts_rank.
+
+        Args:
+            query_text: Search query text
+            top_k: Number of results to return
+            user_id: Optional user ID to filter documents by owner
+
+        Returns:
+            List of dicts with chunk info and BM25 scores
+        """
+        try:
+            # Clean query text for tsquery
+            # Remove special characters that could break tsquery
+            cleaned_query = query_text.replace("'", "").replace('"', "")
+
+            # Build query with ts_rank for BM25-style scoring
+            query = (
+                self.db.query(
+                    DocumentChunk.id,
+                    DocumentChunk.content,
+                    DocumentChunk.chunk_metadata,
+                    DocumentChunk.document_id,
+                    Document.file_name,
+                    # ts_rank scores based on term frequency and document statistics
+                    func.ts_rank(
+                        DocumentChunk.content_tsvector,
+                        func.plainto_tsquery("english", cleaned_query),
+                    ).label("rank"),
+                )
+                .join(Document, DocumentChunk.document_id == Document.id)
+                .filter(Document.status == "completed")
+                .filter(
+                    DocumentChunk.content_tsvector.op("@@")(
+                        func.plainto_tsquery("english", cleaned_query)
+                    )
+                )
+            )
+
+            # Filter by user if specified
+            if user_id:
+                query = query.filter(Document.user_id == user_id)
+
+            # Order by rank (highest first) and limit
+            results = query.order_by(text("rank DESC")).limit(top_k).all()
+
+            chunks = []
+            for row in results:
+                chunks.append(
+                    {
+                        "chunk_id": str(row.id),
+                        "content": row.content,
+                        "metadata": row.chunk_metadata or {},
+                        "document_id": str(row.document_id),
+                        "file_name": row.file_name,
+                        "score": float(row.rank),
+                        "search_type": "bm25",
+                    }
+                )
+
+            logger.info(f"BM25 search returned {len(chunks)} results")
+            return chunks
+
+        except Exception as e:
+            logger.error(f"BM25 search failed: {e}")
+            raise
+
+    def hybrid_search(
+        self,
+        query_text: str,
+        top_k: int,
+        user_id: UUID | None = None,
+        semantic_ratio: float | None = None,
+    ) -> list[dict[str, Any]]:
+        """
+        Perform hybrid search combining semantic and BM25 results.
+
+        Args:
+            query_text: Search query text
+            top_k: Total number of results to return
+            user_id: Optional user ID to filter documents by owner
+            semantic_ratio: Weight for semantic search (0-1). If None, uses config default
+
+        Returns:
+            List of dicts with chunk info and combined scores, sorted by score
+        """
+        try:
+            # Use config default if not specified
+            semantic_ratio = semantic_ratio or settings.SEMANTIC_SEARCH_RATIO
+            bm25_ratio = 1.0 - semantic_ratio
+
+            logger.info(
+                f"Hybrid search: semantic_ratio={semantic_ratio}, bm25_ratio={bm25_ratio}"
+            )
+
+            # Generate query embedding for semantic search with throttling handling
+            try:
+                query_embedding = self.bedrock_client.generate_query_embedding(query_text)
+            except Exception as e:
+                # If embedding generation fails due to throttling, fall back to BM25 only
+                if "ThrottlingException" in str(e) or "Too many requests" in str(e):
+                    logger.warning(
+                        "Throttling detected during query embedding generation, "
+                        "falling back to BM25-only search"
+                    )
+                    # Fall back to BM25-only search
+                    bm25_results = self.bm25_search(query_text, top_k, user_id)
+                    return bm25_results
+                else:
+                    # Re-raise other errors
+                    raise
+
+            # Get more results from each search to ensure good coverage
+            # We'll retrieve 2x top_k from each and then merge
+            retrieve_k = top_k * 2
+
+            # Perform both searches in parallel (conceptually)
+            semantic_results = self.semantic_search(query_embedding, retrieve_k, user_id)
+            bm25_results = self.bm25_search(query_text, retrieve_k, user_id)
+
+            # Normalize scores for both result sets
+            semantic_normalized = self._normalize_scores(semantic_results)
+            bm25_normalized = self._normalize_scores(bm25_results)
+
+            # Merge results with weighted scores
+            merged_results = self._merge_results(
+                semantic_normalized,
+                bm25_normalized,
+                semantic_ratio,
+                bm25_ratio,
+            )
+
+            # Sort by combined score and take top_k
+            merged_results.sort(key=lambda x: x["score"], reverse=True)
+            top_results = merged_results[:top_k]
+
+            # Filter by relevance threshold to avoid returning irrelevant documents
+            final_results = [
+                r for r in top_results if r["score"] >= settings.RELEVANCE_THRESHOLD
+            ]
+
+            logger.info(
+                f"Hybrid search returned {len(final_results)} results "
+                f"(from {len(semantic_results)} semantic + {len(bm25_results)} BM25, "
+                f"filtered by threshold {settings.RELEVANCE_THRESHOLD})"
+            )
+
+            return final_results
+
+        except Exception as e:
+            logger.error(f"Hybrid search failed: {e}")
+            raise
+
+    def _normalize_scores(self, results: list[dict[str, Any]]) -> list[dict[str, Any]]:
+        """
+        Normalize scores to 0-1 range using min-max normalization.
+
+        Args:
+            results: List of search results with scores
+
+        Returns:
+            Results with normalized scores
+        """
+        if not results:
+            return results
+
+        scores = [r["score"] for r in results]
+        min_score = min(scores)
+        max_score = max(scores)
+
+        # Avoid division by zero
+        score_range = max_score - min_score
+        if score_range == 0:
+            # All scores are the same, set normalized to 1.0
+            for r in results:
+                r["normalized_score"] = 1.0
+        else:
+            for r in results:
+                r["normalized_score"] = (r["score"] - min_score) / score_range
+
+        return results
+
+    def _merge_results(
+        self,
+        semantic_results: list[dict[str, Any]],
+        bm25_results: list[dict[str, Any]],
+        semantic_weight: float,
+        bm25_weight: float,
+    ) -> list[dict[str, Any]]:
+        """
+        Merge semantic and BM25 results with weighted scores.
+
+        Args:
+            semantic_results: Results from semantic search (with normalized scores)
+            bm25_results: Results from BM25 search (with normalized scores)
+            semantic_weight: Weight for semantic scores
+            bm25_weight: Weight for BM25 scores
+
+        Returns:
+            Merged results with combined scores
+        """
+        # Create lookup dictionaries for fast merging
+        semantic_lookup = {r["chunk_id"]: r for r in semantic_results}
+        bm25_lookup = {r["chunk_id"]: r for r in bm25_results}
+
+        # Get all unique chunk IDs
+        all_chunk_ids = set(semantic_lookup.keys()) | set(bm25_lookup.keys())
+
+        merged = []
+        for chunk_id in all_chunk_ids:
+            semantic_result = semantic_lookup.get(chunk_id)
+            bm25_result = bm25_lookup.get(chunk_id)
+
+            # Calculate weighted combined score
+            semantic_score = (
+                semantic_result.get("normalized_score", 0.0) if semantic_result else 0.0
+            )
+            bm25_score = bm25_result.get("normalized_score", 0.0) if bm25_result else 0.0
+
+            combined_score = (semantic_weight * semantic_score) + (bm25_weight * bm25_score)
+
+            # Use the result that exists (prefer semantic if both exist)
+            base_result = semantic_result or bm25_result
+
+            # Create merged result
+            merged_result = {
+                "chunk_id": chunk_id,
+                "content": base_result["content"],
+                "metadata": base_result["metadata"],
+                "document_id": base_result["document_id"],
+                "file_name": base_result["file_name"],
+                "score": combined_score,
+                "semantic_score": semantic_score,
+                "bm25_score": bm25_score,
+                "search_type": "hybrid",
+            }
+
+            merged.append(merged_result)
+
+        return merged
+
+
+def get_retrieval_service(db: Session) -> RetrievalService:
+    """
+    Factory function to get a RetrievalService instance.
+
+    Args:
+        db: Database session
+
+    Returns:
+        RetrievalService instance
+    """
+    return RetrievalService(db)
diff --git a/uv.lock b/uv.lock
index 963ae94..33d649e 100644
--- a/uv.lock
+++ b/uv.lock
@@ -1107,6 +1107,8 @@ dependencies = [
     { name = "psycopg2-binary" },
     { name = "pydantic" },
     { name = "pydantic-settings" },
+    { name = "pypdf" },
+    { name = "python-docx" },
     { name = "python-dotenv" },
     { name = "python-jose", extra = ["cryptography"] },
     { name = "python-multipart" },
@@ -1140,9 +1142,11 @@ requires-dist = [
     { name = "psycopg2-binary", specifier = ">=2.9.9" },
     { name = "pydantic", specifier = ">=2.5.0" },
     { name = "pydantic-settings", specifier = ">=2.1.0" },
+    { name = "pypdf", specifier = ">=4.0.0" },
     { name = "pytest", marker = "extra == 'dev'", specifier = ">=7.4.0" },
     { name = "pytest-asyncio", marker = "extra == 'dev'", specifier = ">=0.23.0" },
     { name = "pytest-cov", marker = "extra == 'dev'", specifier = ">=4.1.0" },
+    { name = "python-docx", specifier = ">=1.1.0" },
     { name = "python-dotenv", specifier = ">=1.0.0" },
     { name = "python-jose", extras = ["cryptography"], specifier = ">=3.3.0" },
     { name = "python-multipart", specifier = ">=0.0.6" },
@@ -1489,6 +1493,108 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/80/1a/0c84f7096d41d64425d29db549c8d6fe075f925a5f2022e8087d01d862c2/langsmith-0.4.47-py3-none-any.whl", hash = "sha256:b9e514611d4e1570e33595d33ccb1fe6eda9f96c5f961095a138651f746c1ef5", size = 411207, upload-time = "2025-11-24T16:01:59.123Z" },
 ]
 
+[[package]]
+name = "lxml"
+version = "6.0.2"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/aa/88/262177de60548e5a2bfc46ad28232c9e9cbde697bd94132aeb80364675cb/lxml-6.0.2.tar.gz", hash = "sha256:cd79f3367bd74b317dda655dc8fcfa304d9eb6e4fb06b7168c5cf27f96e0cd62", size = 4073426, upload-time = "2025-09-22T04:04:59.287Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/77/d5/becbe1e2569b474a23f0c672ead8a29ac50b2dc1d5b9de184831bda8d14c/lxml-6.0.2-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:13e35cbc684aadf05d8711a5d1b5857c92e5e580efa9a0d2be197199c8def607", size = 8634365, upload-time = "2025-09-22T04:00:45.672Z" },
+    { url = "https://files.pythonhosted.org/packages/28/66/1ced58f12e804644426b85d0bb8a4478ca77bc1761455da310505f1a3526/lxml-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:3b1675e096e17c6fe9c0e8c81434f5736c0739ff9ac6123c87c2d452f48fc938", size = 4650793, upload-time = "2025-09-22T04:00:47.783Z" },
+    { url = "https://files.pythonhosted.org/packages/11/84/549098ffea39dfd167e3f174b4ce983d0eed61f9d8d25b7bf2a57c3247fc/lxml-6.0.2-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:8ac6e5811ae2870953390452e3476694196f98d447573234592d30488147404d", size = 4944362, upload-time = "2025-09-22T04:00:49.845Z" },
+    { url = "https://files.pythonhosted.org/packages/ac/bd/f207f16abf9749d2037453d56b643a7471d8fde855a231a12d1e095c4f01/lxml-6.0.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:5aa0fc67ae19d7a64c3fe725dc9a1bb11f80e01f78289d05c6f62545affec438", size = 5083152, upload-time = "2025-09-22T04:00:51.709Z" },
+    { url = "https://files.pythonhosted.org/packages/15/ae/bd813e87d8941d52ad5b65071b1affb48da01c4ed3c9c99e40abb266fbff/lxml-6.0.2-cp311-cp311-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:de496365750cc472b4e7902a485d3f152ecf57bd3ba03ddd5578ed8ceb4c5964", size = 5023539, upload-time = "2025-09-22T04:00:53.593Z" },
+    { url = "https://files.pythonhosted.org/packages/02/cd/9bfef16bd1d874fbe0cb51afb00329540f30a3283beb9f0780adbb7eec03/lxml-6.0.2-cp311-cp311-manylinux_2_26_i686.manylinux_2_28_i686.whl", hash = "sha256:200069a593c5e40b8f6fc0d84d86d970ba43138c3e68619ffa234bc9bb806a4d", size = 5344853, upload-time = "2025-09-22T04:00:55.524Z" },
+    { url = "https://files.pythonhosted.org/packages/b8/89/ea8f91594bc5dbb879734d35a6f2b0ad50605d7fb419de2b63d4211765cc/lxml-6.0.2-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:7d2de809c2ee3b888b59f995625385f74629707c9355e0ff856445cdcae682b7", size = 5225133, upload-time = "2025-09-22T04:00:57.269Z" },
+    { url = "https://files.pythonhosted.org/packages/b9/37/9c735274f5dbec726b2db99b98a43950395ba3d4a1043083dba2ad814170/lxml-6.0.2-cp311-cp311-manylinux_2_31_armv7l.whl", hash = "sha256:b2c3da8d93cf5db60e8858c17684c47d01fee6405e554fb55018dd85fc23b178", size = 4677944, upload-time = "2025-09-22T04:00:59.052Z" },
+    { url = "https://files.pythonhosted.org/packages/20/28/7dfe1ba3475d8bfca3878365075abe002e05d40dfaaeb7ec01b4c587d533/lxml-6.0.2-cp311-cp311-manylinux_2_38_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:442de7530296ef5e188373a1ea5789a46ce90c4847e597856570439621d9c553", size = 5284535, upload-time = "2025-09-22T04:01:01.335Z" },
+    { url = "https://files.pythonhosted.org/packages/e7/cf/5f14bc0de763498fc29510e3532bf2b4b3a1c1d5d0dff2e900c16ba021ef/lxml-6.0.2-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:2593c77efde7bfea7f6389f1ab249b15ed4aa5bc5cb5131faa3b843c429fbedb", size = 5067343, upload-time = "2025-09-22T04:01:03.13Z" },
+    { url = "https://files.pythonhosted.org/packages/1c/b0/bb8275ab5472f32b28cfbbcc6db7c9d092482d3439ca279d8d6fa02f7025/lxml-6.0.2-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:3e3cb08855967a20f553ff32d147e14329b3ae70ced6edc2f282b94afbc74b2a", size = 4725419, upload-time = "2025-09-22T04:01:05.013Z" },
+    { url = "https://files.pythonhosted.org/packages/25/4c/7c222753bc72edca3b99dbadba1b064209bc8ed4ad448af990e60dcce462/lxml-6.0.2-cp311-cp311-musllinux_1_2_riscv64.whl", hash = "sha256:2ed6c667fcbb8c19c6791bbf40b7268ef8ddf5a96940ba9404b9f9a304832f6c", size = 5275008, upload-time = "2025-09-22T04:01:07.327Z" },
+    { url = "https://files.pythonhosted.org/packages/6c/8c/478a0dc6b6ed661451379447cdbec77c05741a75736d97e5b2b729687828/lxml-6.0.2-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:b8f18914faec94132e5b91e69d76a5c1d7b0c73e2489ea8929c4aaa10b76bbf7", size = 5248906, upload-time = "2025-09-22T04:01:09.452Z" },
+    { url = "https://files.pythonhosted.org/packages/2d/d9/5be3a6ab2784cdf9accb0703b65e1b64fcdd9311c9f007630c7db0cfcce1/lxml-6.0.2-cp311-cp311-win32.whl", hash = "sha256:6605c604e6daa9e0d7f0a2137bdc47a2e93b59c60a65466353e37f8272f47c46", size = 3610357, upload-time = "2025-09-22T04:01:11.102Z" },
+    { url = "https://files.pythonhosted.org/packages/e2/7d/ca6fb13349b473d5732fb0ee3eec8f6c80fc0688e76b7d79c1008481bf1f/lxml-6.0.2-cp311-cp311-win_amd64.whl", hash = "sha256:e5867f2651016a3afd8dd2c8238baa66f1e2802f44bc17e236f547ace6647078", size = 4036583, upload-time = "2025-09-22T04:01:12.766Z" },
+    { url = "https://files.pythonhosted.org/packages/ab/a2/51363b5ecd3eab46563645f3a2c3836a2fc67d01a1b87c5017040f39f567/lxml-6.0.2-cp311-cp311-win_arm64.whl", hash = "sha256:4197fb2534ee05fd3e7afaab5d8bfd6c2e186f65ea7f9cd6a82809c887bd1285", size = 3680591, upload-time = "2025-09-22T04:01:14.874Z" },
+    { url = "https://files.pythonhosted.org/packages/f3/c8/8ff2bc6b920c84355146cd1ab7d181bc543b89241cfb1ebee824a7c81457/lxml-6.0.2-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:a59f5448ba2ceccd06995c95ea59a7674a10de0810f2ce90c9006f3cbc044456", size = 8661887, upload-time = "2025-09-22T04:01:17.265Z" },
+    { url = "https://files.pythonhosted.org/packages/37/6f/9aae1008083bb501ef63284220ce81638332f9ccbfa53765b2b7502203cf/lxml-6.0.2-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:e8113639f3296706fbac34a30813929e29247718e88173ad849f57ca59754924", size = 4667818, upload-time = "2025-09-22T04:01:19.688Z" },
+    { url = "https://files.pythonhosted.org/packages/f1/ca/31fb37f99f37f1536c133476674c10b577e409c0a624384147653e38baf2/lxml-6.0.2-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:a8bef9b9825fa8bc816a6e641bb67219489229ebc648be422af695f6e7a4fa7f", size = 4950807, upload-time = "2025-09-22T04:01:21.487Z" },
+    { url = "https://files.pythonhosted.org/packages/da/87/f6cb9442e4bada8aab5ae7e1046264f62fdbeaa6e3f6211b93f4c0dd97f1/lxml-6.0.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:65ea18d710fd14e0186c2f973dc60bb52039a275f82d3c44a0e42b43440ea534", size = 5109179, upload-time = "2025-09-22T04:01:23.32Z" },
+    { url = "https://files.pythonhosted.org/packages/c8/20/a7760713e65888db79bbae4f6146a6ae5c04e4a204a3c48896c408cd6ed2/lxml-6.0.2-cp312-cp312-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:c371aa98126a0d4c739ca93ceffa0fd7a5d732e3ac66a46e74339acd4d334564", size = 5023044, upload-time = "2025-09-22T04:01:25.118Z" },
+    { url = "https://files.pythonhosted.org/packages/a2/b0/7e64e0460fcb36471899f75831509098f3fd7cd02a3833ac517433cb4f8f/lxml-6.0.2-cp312-cp312-manylinux_2_26_i686.manylinux_2_28_i686.whl", hash = "sha256:700efd30c0fa1a3581d80a748157397559396090a51d306ea59a70020223d16f", size = 5359685, upload-time = "2025-09-22T04:01:27.398Z" },
+    { url = "https://files.pythonhosted.org/packages/b9/e1/e5df362e9ca4e2f48ed6411bd4b3a0ae737cc842e96877f5bf9428055ab4/lxml-6.0.2-cp312-cp312-manylinux_2_26_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:c33e66d44fe60e72397b487ee92e01da0d09ba2d66df8eae42d77b6d06e5eba0", size = 5654127, upload-time = "2025-09-22T04:01:29.629Z" },
+    { url = "https://files.pythonhosted.org/packages/c6/d1/232b3309a02d60f11e71857778bfcd4acbdb86c07db8260caf7d008b08f8/lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:90a345bbeaf9d0587a3aaffb7006aa39ccb6ff0e96a57286c0cb2fd1520ea192", size = 5253958, upload-time = "2025-09-22T04:01:31.535Z" },
+    { url = "https://files.pythonhosted.org/packages/35/35/d955a070994725c4f7d80583a96cab9c107c57a125b20bb5f708fe941011/lxml-6.0.2-cp312-cp312-manylinux_2_31_armv7l.whl", hash = "sha256:064fdadaf7a21af3ed1dcaa106b854077fbeada827c18f72aec9346847cd65d0", size = 4711541, upload-time = "2025-09-22T04:01:33.801Z" },
+    { url = "https://files.pythonhosted.org/packages/1e/be/667d17363b38a78c4bd63cfd4b4632029fd68d2c2dc81f25ce9eb5224dd5/lxml-6.0.2-cp312-cp312-manylinux_2_38_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:fbc74f42c3525ac4ffa4b89cbdd00057b6196bcefe8bce794abd42d33a018092", size = 5267426, upload-time = "2025-09-22T04:01:35.639Z" },
+    { url = "https://files.pythonhosted.org/packages/ea/47/62c70aa4a1c26569bc958c9ca86af2bb4e1f614e8c04fb2989833874f7ae/lxml-6.0.2-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:6ddff43f702905a4e32bc24f3f2e2edfe0f8fde3277d481bffb709a4cced7a1f", size = 5064917, upload-time = "2025-09-22T04:01:37.448Z" },
+    { url = "https://files.pythonhosted.org/packages/bd/55/6ceddaca353ebd0f1908ef712c597f8570cc9c58130dbb89903198e441fd/lxml-6.0.2-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:6da5185951d72e6f5352166e3da7b0dc27aa70bd1090b0eb3f7f7212b53f1bb8", size = 4788795, upload-time = "2025-09-22T04:01:39.165Z" },
+    { url = "https://files.pythonhosted.org/packages/cf/e8/fd63e15da5e3fd4c2146f8bbb3c14e94ab850589beab88e547b2dbce22e1/lxml-6.0.2-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:57a86e1ebb4020a38d295c04fc79603c7899e0df71588043eb218722dabc087f", size = 5676759, upload-time = "2025-09-22T04:01:41.506Z" },
+    { url = "https://files.pythonhosted.org/packages/76/47/b3ec58dc5c374697f5ba37412cd2728f427d056315d124dd4b61da381877/lxml-6.0.2-cp312-cp312-musllinux_1_2_riscv64.whl", hash = "sha256:2047d8234fe735ab77802ce5f2297e410ff40f5238aec569ad7c8e163d7b19a6", size = 5255666, upload-time = "2025-09-22T04:01:43.363Z" },
+    { url = "https://files.pythonhosted.org/packages/19/93/03ba725df4c3d72afd9596eef4a37a837ce8e4806010569bedfcd2cb68fd/lxml-6.0.2-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:6f91fd2b2ea15a6800c8e24418c0775a1694eefc011392da73bc6cef2623b322", size = 5277989, upload-time = "2025-09-22T04:01:45.215Z" },
+    { url = "https://files.pythonhosted.org/packages/c6/80/c06de80bfce881d0ad738576f243911fccf992687ae09fd80b734712b39c/lxml-6.0.2-cp312-cp312-win32.whl", hash = "sha256:3ae2ce7d6fedfb3414a2b6c5e20b249c4c607f72cb8d2bb7cc9c6ec7c6f4e849", size = 3611456, upload-time = "2025-09-22T04:01:48.243Z" },
+    { url = "https://files.pythonhosted.org/packages/f7/d7/0cdfb6c3e30893463fb3d1e52bc5f5f99684a03c29a0b6b605cfae879cd5/lxml-6.0.2-cp312-cp312-win_amd64.whl", hash = "sha256:72c87e5ee4e58a8354fb9c7c84cbf95a1c8236c127a5d1b7683f04bed8361e1f", size = 4011793, upload-time = "2025-09-22T04:01:50.042Z" },
+    { url = "https://files.pythonhosted.org/packages/ea/7b/93c73c67db235931527301ed3785f849c78991e2e34f3fd9a6663ffda4c5/lxml-6.0.2-cp312-cp312-win_arm64.whl", hash = "sha256:61cb10eeb95570153e0c0e554f58df92ecf5109f75eacad4a95baa709e26c3d6", size = 3672836, upload-time = "2025-09-22T04:01:52.145Z" },
+    { url = "https://files.pythonhosted.org/packages/53/fd/4e8f0540608977aea078bf6d79f128e0e2c2bba8af1acf775c30baa70460/lxml-6.0.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:9b33d21594afab46f37ae58dfadd06636f154923c4e8a4d754b0127554eb2e77", size = 8648494, upload-time = "2025-09-22T04:01:54.242Z" },
+    { url = "https://files.pythonhosted.org/packages/5d/f4/2a94a3d3dfd6c6b433501b8d470a1960a20ecce93245cf2db1706adf6c19/lxml-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:6c8963287d7a4c5c9a432ff487c52e9c5618667179c18a204bdedb27310f022f", size = 4661146, upload-time = "2025-09-22T04:01:56.282Z" },
+    { url = "https://files.pythonhosted.org/packages/25/2e/4efa677fa6b322013035d38016f6ae859d06cac67437ca7dc708a6af7028/lxml-6.0.2-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:1941354d92699fb5ffe6ed7b32f9649e43c2feb4b97205f75866f7d21aa91452", size = 4946932, upload-time = "2025-09-22T04:01:58.989Z" },
+    { url = "https://files.pythonhosted.org/packages/ce/0f/526e78a6d38d109fdbaa5049c62e1d32fdd70c75fb61c4eadf3045d3d124/lxml-6.0.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:bb2f6ca0ae2d983ded09357b84af659c954722bbf04dea98030064996d156048", size = 5100060, upload-time = "2025-09-22T04:02:00.812Z" },
+    { url = "https://files.pythonhosted.org/packages/81/76/99de58d81fa702cc0ea7edae4f4640416c2062813a00ff24bd70ac1d9c9b/lxml-6.0.2-cp313-cp313-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:eb2a12d704f180a902d7fa778c6d71f36ceb7b0d317f34cdc76a5d05aa1dd1df", size = 5019000, upload-time = "2025-09-22T04:02:02.671Z" },
+    { url = "https://files.pythonhosted.org/packages/b5/35/9e57d25482bc9a9882cb0037fdb9cc18f4b79d85df94fa9d2a89562f1d25/lxml-6.0.2-cp313-cp313-manylinux_2_26_i686.manylinux_2_28_i686.whl", hash = "sha256:6ec0e3f745021bfed19c456647f0298d60a24c9ff86d9d051f52b509663feeb1", size = 5348496, upload-time = "2025-09-22T04:02:04.904Z" },
+    { url = "https://files.pythonhosted.org/packages/a6/8e/cb99bd0b83ccc3e8f0f528e9aa1f7a9965dfec08c617070c5db8d63a87ce/lxml-6.0.2-cp313-cp313-manylinux_2_26_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:846ae9a12d54e368933b9759052d6206a9e8b250291109c48e350c1f1f49d916", size = 5643779, upload-time = "2025-09-22T04:02:06.689Z" },
+    { url = "https://files.pythonhosted.org/packages/d0/34/9e591954939276bb679b73773836c6684c22e56d05980e31d52a9a8deb18/lxml-6.0.2-cp313-cp313-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:ef9266d2aa545d7374938fb5c484531ef5a2ec7f2d573e62f8ce722c735685fd", size = 5244072, upload-time = "2025-09-22T04:02:08.587Z" },
+    { url = "https://files.pythonhosted.org/packages/8d/27/b29ff065f9aaca443ee377aff699714fcbffb371b4fce5ac4ca759e436d5/lxml-6.0.2-cp313-cp313-manylinux_2_31_armv7l.whl", hash = "sha256:4077b7c79f31755df33b795dc12119cb557a0106bfdab0d2c2d97bd3cf3dffa6", size = 4718675, upload-time = "2025-09-22T04:02:10.783Z" },
+    { url = "https://files.pythonhosted.org/packages/2b/9f/f756f9c2cd27caa1a6ef8c32ae47aadea697f5c2c6d07b0dae133c244fbe/lxml-6.0.2-cp313-cp313-manylinux_2_38_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:a7c5d5e5f1081955358533be077166ee97ed2571d6a66bdba6ec2f609a715d1a", size = 5255171, upload-time = "2025-09-22T04:02:12.631Z" },
+    { url = "https://files.pythonhosted.org/packages/61/46/bb85ea42d2cb1bd8395484fd72f38e3389611aa496ac7772da9205bbda0e/lxml-6.0.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:8f8d0cbd0674ee89863a523e6994ac25fd5be9c8486acfc3e5ccea679bad2679", size = 5057175, upload-time = "2025-09-22T04:02:14.718Z" },
+    { url = "https://files.pythonhosted.org/packages/95/0c/443fc476dcc8e41577f0af70458c50fe299a97bb6b7505bb1ae09aa7f9ac/lxml-6.0.2-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:2cbcbf6d6e924c28f04a43f3b6f6e272312a090f269eff68a2982e13e5d57659", size = 4785688, upload-time = "2025-09-22T04:02:16.957Z" },
+    { url = "https://files.pythonhosted.org/packages/48/78/6ef0b359d45bb9697bc5a626e1992fa5d27aa3f8004b137b2314793b50a0/lxml-6.0.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:dfb874cfa53340009af6bdd7e54ebc0d21012a60a4e65d927c2e477112e63484", size = 5660655, upload-time = "2025-09-22T04:02:18.815Z" },
+    { url = "https://files.pythonhosted.org/packages/ff/ea/e1d33808f386bc1339d08c0dcada6e4712d4ed8e93fcad5f057070b7988a/lxml-6.0.2-cp313-cp313-musllinux_1_2_riscv64.whl", hash = "sha256:fb8dae0b6b8b7f9e96c26fdd8121522ce5de9bb5538010870bd538683d30e9a2", size = 5247695, upload-time = "2025-09-22T04:02:20.593Z" },
+    { url = "https://files.pythonhosted.org/packages/4f/47/eba75dfd8183673725255247a603b4ad606f4ae657b60c6c145b381697da/lxml-6.0.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:358d9adae670b63e95bc59747c72f4dc97c9ec58881d4627fe0120da0f90d314", size = 5269841, upload-time = "2025-09-22T04:02:22.489Z" },
+    { url = "https://files.pythonhosted.org/packages/76/04/5c5e2b8577bc936e219becb2e98cdb1aca14a4921a12995b9d0c523502ae/lxml-6.0.2-cp313-cp313-win32.whl", hash = "sha256:e8cd2415f372e7e5a789d743d133ae474290a90b9023197fd78f32e2dc6873e2", size = 3610700, upload-time = "2025-09-22T04:02:24.465Z" },
+    { url = "https://files.pythonhosted.org/packages/fe/0a/4643ccc6bb8b143e9f9640aa54e38255f9d3b45feb2cbe7ae2ca47e8782e/lxml-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:b30d46379644fbfc3ab81f8f82ae4de55179414651f110a1514f0b1f8f6cb2d7", size = 4010347, upload-time = "2025-09-22T04:02:26.286Z" },
+    { url = "https://files.pythonhosted.org/packages/31/ef/dcf1d29c3f530577f61e5fe2f1bd72929acf779953668a8a47a479ae6f26/lxml-6.0.2-cp313-cp313-win_arm64.whl", hash = "sha256:13dcecc9946dca97b11b7c40d29fba63b55ab4170d3c0cf8c0c164343b9bfdcf", size = 3671248, upload-time = "2025-09-22T04:02:27.918Z" },
+    { url = "https://files.pythonhosted.org/packages/03/15/d4a377b385ab693ce97b472fe0c77c2b16ec79590e688b3ccc71fba19884/lxml-6.0.2-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:b0c732aa23de8f8aec23f4b580d1e52905ef468afb4abeafd3fec77042abb6fe", size = 8659801, upload-time = "2025-09-22T04:02:30.113Z" },
+    { url = "https://files.pythonhosted.org/packages/c8/e8/c128e37589463668794d503afaeb003987373c5f94d667124ffd8078bbd9/lxml-6.0.2-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:4468e3b83e10e0317a89a33d28f7aeba1caa4d1a6fd457d115dd4ffe90c5931d", size = 4659403, upload-time = "2025-09-22T04:02:32.119Z" },
+    { url = "https://files.pythonhosted.org/packages/00/ce/74903904339decdf7da7847bb5741fc98a5451b42fc419a86c0c13d26fe2/lxml-6.0.2-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:abd44571493973bad4598a3be7e1d807ed45aa2adaf7ab92ab7c62609569b17d", size = 4966974, upload-time = "2025-09-22T04:02:34.155Z" },
+    { url = "https://files.pythonhosted.org/packages/1f/d3/131dec79ce61c5567fecf82515bd9bc36395df42501b50f7f7f3bd065df0/lxml-6.0.2-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:370cd78d5855cfbffd57c422851f7d3864e6ae72d0da615fca4dad8c45d375a5", size = 5102953, upload-time = "2025-09-22T04:02:36.054Z" },
+    { url = "https://files.pythonhosted.org/packages/3a/ea/a43ba9bb750d4ffdd885f2cd333572f5bb900cd2408b67fdda07e85978a0/lxml-6.0.2-cp314-cp314-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:901e3b4219fa04ef766885fb40fa516a71662a4c61b80c94d25336b4934b71c0", size = 5055054, upload-time = "2025-09-22T04:02:38.154Z" },
+    { url = "https://files.pythonhosted.org/packages/60/23/6885b451636ae286c34628f70a7ed1fcc759f8d9ad382d132e1c8d3d9bfd/lxml-6.0.2-cp314-cp314-manylinux_2_26_i686.manylinux_2_28_i686.whl", hash = "sha256:a4bf42d2e4cf52c28cc1812d62426b9503cdb0c87a6de81442626aa7d69707ba", size = 5352421, upload-time = "2025-09-22T04:02:40.413Z" },
+    { url = "https://files.pythonhosted.org/packages/48/5b/fc2ddfc94ddbe3eebb8e9af6e3fd65e2feba4967f6a4e9683875c394c2d8/lxml-6.0.2-cp314-cp314-manylinux_2_26_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:b2c7fdaa4d7c3d886a42534adec7cfac73860b89b4e5298752f60aa5984641a0", size = 5673684, upload-time = "2025-09-22T04:02:42.288Z" },
+    { url = "https://files.pythonhosted.org/packages/29/9c/47293c58cc91769130fbf85531280e8cc7868f7fbb6d92f4670071b9cb3e/lxml-6.0.2-cp314-cp314-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:98a5e1660dc7de2200b00d53fa00bcd3c35a3608c305d45a7bbcaf29fa16e83d", size = 5252463, upload-time = "2025-09-22T04:02:44.165Z" },
+    { url = "https://files.pythonhosted.org/packages/9b/da/ba6eceb830c762b48e711ded880d7e3e89fc6c7323e587c36540b6b23c6b/lxml-6.0.2-cp314-cp314-manylinux_2_31_armv7l.whl", hash = "sha256:dc051506c30b609238d79eda75ee9cab3e520570ec8219844a72a46020901e37", size = 4698437, upload-time = "2025-09-22T04:02:46.524Z" },
+    { url = "https://files.pythonhosted.org/packages/a5/24/7be3f82cb7990b89118d944b619e53c656c97dc89c28cfb143fdb7cd6f4d/lxml-6.0.2-cp314-cp314-manylinux_2_38_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:8799481bbdd212470d17513a54d568f44416db01250f49449647b5ab5b5dccb9", size = 5269890, upload-time = "2025-09-22T04:02:48.812Z" },
+    { url = "https://files.pythonhosted.org/packages/1b/bd/dcfb9ea1e16c665efd7538fc5d5c34071276ce9220e234217682e7d2c4a5/lxml-6.0.2-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:9261bb77c2dab42f3ecd9103951aeca2c40277701eb7e912c545c1b16e0e4917", size = 5097185, upload-time = "2025-09-22T04:02:50.746Z" },
+    { url = "https://files.pythonhosted.org/packages/21/04/a60b0ff9314736316f28316b694bccbbabe100f8483ad83852d77fc7468e/lxml-6.0.2-cp314-cp314-musllinux_1_2_armv7l.whl", hash = "sha256:65ac4a01aba353cfa6d5725b95d7aed6356ddc0a3cd734de00124d285b04b64f", size = 4745895, upload-time = "2025-09-22T04:02:52.968Z" },
+    { url = "https://files.pythonhosted.org/packages/d6/bd/7d54bd1846e5a310d9c715921c5faa71cf5c0853372adf78aee70c8d7aa2/lxml-6.0.2-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:b22a07cbb82fea98f8a2fd814f3d1811ff9ed76d0fc6abc84eb21527596e7cc8", size = 5695246, upload-time = "2025-09-22T04:02:54.798Z" },
+    { url = "https://files.pythonhosted.org/packages/fd/32/5643d6ab947bc371da21323acb2a6e603cedbe71cb4c99c8254289ab6f4e/lxml-6.0.2-cp314-cp314-musllinux_1_2_riscv64.whl", hash = "sha256:d759cdd7f3e055d6bc8d9bec3ad905227b2e4c785dc16c372eb5b5e83123f48a", size = 5260797, upload-time = "2025-09-22T04:02:57.058Z" },
+    { url = "https://files.pythonhosted.org/packages/33/da/34c1ec4cff1eea7d0b4cd44af8411806ed943141804ac9c5d565302afb78/lxml-6.0.2-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:945da35a48d193d27c188037a05fec5492937f66fb1958c24fc761fb9d40d43c", size = 5277404, upload-time = "2025-09-22T04:02:58.966Z" },
+    { url = "https://files.pythonhosted.org/packages/82/57/4eca3e31e54dc89e2c3507e1cd411074a17565fa5ffc437c4ae0a00d439e/lxml-6.0.2-cp314-cp314-win32.whl", hash = "sha256:be3aaa60da67e6153eb15715cc2e19091af5dc75faef8b8a585aea372507384b", size = 3670072, upload-time = "2025-09-22T04:03:38.05Z" },
+    { url = "https://files.pythonhosted.org/packages/e3/e0/c96cf13eccd20c9421ba910304dae0f619724dcf1702864fd59dd386404d/lxml-6.0.2-cp314-cp314-win_amd64.whl", hash = "sha256:fa25afbadead523f7001caf0c2382afd272c315a033a7b06336da2637d92d6ed", size = 4080617, upload-time = "2025-09-22T04:03:39.835Z" },
+    { url = "https://files.pythonhosted.org/packages/d5/5d/b3f03e22b3d38d6f188ef044900a9b29b2fe0aebb94625ce9fe244011d34/lxml-6.0.2-cp314-cp314-win_arm64.whl", hash = "sha256:063eccf89df5b24e361b123e257e437f9e9878f425ee9aae3144c77faf6da6d8", size = 3754930, upload-time = "2025-09-22T04:03:41.565Z" },
+    { url = "https://files.pythonhosted.org/packages/5e/5c/42c2c4c03554580708fc738d13414801f340c04c3eff90d8d2d227145275/lxml-6.0.2-cp314-cp314t-macosx_10_13_universal2.whl", hash = "sha256:6162a86d86893d63084faaf4ff937b3daea233e3682fb4474db07395794fa80d", size = 8910380, upload-time = "2025-09-22T04:03:01.645Z" },
+    { url = "https://files.pythonhosted.org/packages/bf/4f/12df843e3e10d18d468a7557058f8d3733e8b6e12401f30b1ef29360740f/lxml-6.0.2-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:414aaa94e974e23a3e92e7ca5b97d10c0cf37b6481f50911032c69eeb3991bba", size = 4775632, upload-time = "2025-09-22T04:03:03.814Z" },
+    { url = "https://files.pythonhosted.org/packages/e4/0c/9dc31e6c2d0d418483cbcb469d1f5a582a1cd00a1f4081953d44051f3c50/lxml-6.0.2-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:48461bd21625458dd01e14e2c38dd0aea69addc3c4f960c30d9f59d7f93be601", size = 4975171, upload-time = "2025-09-22T04:03:05.651Z" },
+    { url = "https://files.pythonhosted.org/packages/e7/2b/9b870c6ca24c841bdd887504808f0417aa9d8d564114689266f19ddf29c8/lxml-6.0.2-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:25fcc59afc57d527cfc78a58f40ab4c9b8fd096a9a3f964d2781ffb6eb33f4ed", size = 5110109, upload-time = "2025-09-22T04:03:07.452Z" },
+    { url = "https://files.pythonhosted.org/packages/bf/0c/4f5f2a4dd319a178912751564471355d9019e220c20d7db3fb8307ed8582/lxml-6.0.2-cp314-cp314t-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:5179c60288204e6ddde3f774a93350177e08876eaf3ab78aa3a3649d43eb7d37", size = 5041061, upload-time = "2025-09-22T04:03:09.297Z" },
+    { url = "https://files.pythonhosted.org/packages/12/64/554eed290365267671fe001a20d72d14f468ae4e6acef1e179b039436967/lxml-6.0.2-cp314-cp314t-manylinux_2_26_i686.manylinux_2_28_i686.whl", hash = "sha256:967aab75434de148ec80597b75062d8123cadf2943fb4281f385141e18b21338", size = 5306233, upload-time = "2025-09-22T04:03:11.651Z" },
+    { url = "https://files.pythonhosted.org/packages/7a/31/1d748aa275e71802ad9722df32a7a35034246b42c0ecdd8235412c3396ef/lxml-6.0.2-cp314-cp314t-manylinux_2_26_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:d100fcc8930d697c6561156c6810ab4a508fb264c8b6779e6e61e2ed5e7558f9", size = 5604739, upload-time = "2025-09-22T04:03:13.592Z" },
+    { url = "https://files.pythonhosted.org/packages/8f/41/2c11916bcac09ed561adccacceaedd2bf0e0b25b297ea92aab99fd03d0fa/lxml-6.0.2-cp314-cp314t-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:2ca59e7e13e5981175b8b3e4ab84d7da57993eeff53c07764dcebda0d0e64ecd", size = 5225119, upload-time = "2025-09-22T04:03:15.408Z" },
+    { url = "https://files.pythonhosted.org/packages/99/05/4e5c2873d8f17aa018e6afde417c80cc5d0c33be4854cce3ef5670c49367/lxml-6.0.2-cp314-cp314t-manylinux_2_31_armv7l.whl", hash = "sha256:957448ac63a42e2e49531b9d6c0fa449a1970dbc32467aaad46f11545be9af1d", size = 4633665, upload-time = "2025-09-22T04:03:17.262Z" },
+    { url = "https://files.pythonhosted.org/packages/0f/c9/dcc2da1bebd6275cdc723b515f93edf548b82f36a5458cca3578bc899332/lxml-6.0.2-cp314-cp314t-manylinux_2_38_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:b7fc49c37f1786284b12af63152fe1d0990722497e2d5817acfe7a877522f9a9", size = 5234997, upload-time = "2025-09-22T04:03:19.14Z" },
+    { url = "https://files.pythonhosted.org/packages/9c/e2/5172e4e7468afca64a37b81dba152fc5d90e30f9c83c7c3213d6a02a5ce4/lxml-6.0.2-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:e19e0643cc936a22e837f79d01a550678da8377d7d801a14487c10c34ee49c7e", size = 5090957, upload-time = "2025-09-22T04:03:21.436Z" },
+    { url = "https://files.pythonhosted.org/packages/a5/b3/15461fd3e5cd4ddcb7938b87fc20b14ab113b92312fc97afe65cd7c85de1/lxml-6.0.2-cp314-cp314t-musllinux_1_2_armv7l.whl", hash = "sha256:1db01e5cf14345628e0cbe71067204db658e2fb8e51e7f33631f5f4735fefd8d", size = 4764372, upload-time = "2025-09-22T04:03:23.27Z" },
+    { url = "https://files.pythonhosted.org/packages/05/33/f310b987c8bf9e61c4dd8e8035c416bd3230098f5e3cfa69fc4232de7059/lxml-6.0.2-cp314-cp314t-musllinux_1_2_ppc64le.whl", hash = "sha256:875c6b5ab39ad5291588aed6925fac99d0097af0dd62f33c7b43736043d4a2ec", size = 5634653, upload-time = "2025-09-22T04:03:25.767Z" },
+    { url = "https://files.pythonhosted.org/packages/70/ff/51c80e75e0bc9382158133bdcf4e339b5886c6ee2418b5199b3f1a61ed6d/lxml-6.0.2-cp314-cp314t-musllinux_1_2_riscv64.whl", hash = "sha256:cdcbed9ad19da81c480dfd6dd161886db6096083c9938ead313d94b30aadf272", size = 5233795, upload-time = "2025-09-22T04:03:27.62Z" },
+    { url = "https://files.pythonhosted.org/packages/56/4d/4856e897df0d588789dd844dbed9d91782c4ef0b327f96ce53c807e13128/lxml-6.0.2-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:80dadc234ebc532e09be1975ff538d154a7fa61ea5031c03d25178855544728f", size = 5257023, upload-time = "2025-09-22T04:03:30.056Z" },
+    { url = "https://files.pythonhosted.org/packages/0f/85/86766dfebfa87bea0ab78e9ff7a4b4b45225df4b4d3b8cc3c03c5cd68464/lxml-6.0.2-cp314-cp314t-win32.whl", hash = "sha256:da08e7bb297b04e893d91087df19638dc7a6bb858a954b0cc2b9f5053c922312", size = 3911420, upload-time = "2025-09-22T04:03:32.198Z" },
+    { url = "https://files.pythonhosted.org/packages/fe/1a/b248b355834c8e32614650b8008c69ffeb0ceb149c793961dd8c0b991bb3/lxml-6.0.2-cp314-cp314t-win_amd64.whl", hash = "sha256:252a22982dca42f6155125ac76d3432e548a7625d56f5a273ee78a5057216eca", size = 4406837, upload-time = "2025-09-22T04:03:34.027Z" },
+    { url = "https://files.pythonhosted.org/packages/92/aa/df863bcc39c5e0946263454aba394de8a9084dbaff8ad143846b0d844739/lxml-6.0.2-cp314-cp314t-win_arm64.whl", hash = "sha256:bb4c1847b303835d89d785a18801a883436cdfd5dc3d62947f9c49e24f0f5a2c", size = 3822205, upload-time = "2025-09-22T04:03:36.249Z" },
+    { url = "https://files.pythonhosted.org/packages/0b/11/29d08bc103a62c0eba8016e7ed5aeebbf1e4312e83b0b1648dd203b0e87d/lxml-6.0.2-pp311-pypy311_pp73-macosx_10_15_x86_64.whl", hash = "sha256:1c06035eafa8404b5cf475bb37a9f6088b0aca288d4ccc9d69389750d5543700", size = 3949829, upload-time = "2025-09-22T04:04:45.608Z" },
+    { url = "https://files.pythonhosted.org/packages/12/b3/52ab9a3b31e5ab8238da241baa19eec44d2ab426532441ee607165aebb52/lxml-6.0.2-pp311-pypy311_pp73-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:c7d13103045de1bdd6fe5d61802565f1a3537d70cd3abf596aa0af62761921ee", size = 4226277, upload-time = "2025-09-22T04:04:47.754Z" },
+    { url = "https://files.pythonhosted.org/packages/a0/33/1eaf780c1baad88224611df13b1c2a9dfa460b526cacfe769103ff50d845/lxml-6.0.2-pp311-pypy311_pp73-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:0a3c150a95fbe5ac91de323aa756219ef9cf7fde5a3f00e2281e30f33fa5fa4f", size = 4330433, upload-time = "2025-09-22T04:04:49.907Z" },
+    { url = "https://files.pythonhosted.org/packages/7a/c1/27428a2ff348e994ab4f8777d3a0ad510b6b92d37718e5887d2da99952a2/lxml-6.0.2-pp311-pypy311_pp73-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:60fa43be34f78bebb27812ed90f1925ec99560b0fa1decdb7d12b84d857d31e9", size = 4272119, upload-time = "2025-09-22T04:04:51.801Z" },
+    { url = "https://files.pythonhosted.org/packages/f0/d0/3020fa12bcec4ab62f97aab026d57c2f0cfd480a558758d9ca233bb6a79d/lxml-6.0.2-pp311-pypy311_pp73-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:21c73b476d3cfe836be731225ec3421fa2f048d84f6df6a8e70433dff1376d5a", size = 4417314, upload-time = "2025-09-22T04:04:55.024Z" },
+    { url = "https://files.pythonhosted.org/packages/6c/77/d7f491cbc05303ac6801651aabeb262d43f319288c1ea96c66b1d2692ff3/lxml-6.0.2-pp311-pypy311_pp73-win_amd64.whl", hash = "sha256:27220da5be049e936c3aca06f174e8827ca6445a4353a1995584311487fc4e3e", size = 3518768, upload-time = "2025-09-22T04:04:57.097Z" },
+]
+
 [[package]]
 name = "mako"
 version = "1.3.10"
@@ -2455,6 +2561,15 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/c7/21/705964c7812476f378728bdf590ca4b771ec72385c533964653c68e86bdc/pygments-2.19.2-py3-none-any.whl", hash = "sha256:86540386c03d588bb81d44bc3928634ff26449851e99741617ecb9037ee5ec0b", size = 1225217, upload-time = "2025-06-21T13:39:07.939Z" },
 ]
 
+[[package]]
+name = "pypdf"
+version = "6.4.0"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/f3/01/f7510cc6124f494cfbec2e8d3c2e1a20d4f6c18622b0c03a3a70e968bacb/pypdf-6.4.0.tar.gz", hash = "sha256:4769d471f8ddc3341193ecc5d6560fa44cf8cd0abfabf21af4e195cc0c224072", size = 5276661, upload-time = "2025-11-23T14:04:43.185Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/cd/f2/9c9429411c91ac1dd5cd66780f22b6df20c64c3646cdd1e6d67cf38579c4/pypdf-6.4.0-py3-none-any.whl", hash = "sha256:55ab9837ed97fd7fcc5c131d52fcc2223bc5c6b8a1488bbf7c0e27f1f0023a79", size = 329497, upload-time = "2025-11-23T14:04:41.448Z" },
+]
+
 [[package]]
 name = "pytest"
 version = "9.0.1"
@@ -2510,6 +2625,19 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892, upload-time = "2024-03-01T18:36:18.57Z" },
 ]
 
+[[package]]
+name = "python-docx"
+version = "1.2.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "lxml" },
+    { name = "typing-extensions" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/a9/f7/eddfe33871520adab45aaa1a71f0402a2252050c14c7e3009446c8f4701c/python_docx-1.2.0.tar.gz", hash = "sha256:7bc9d7b7d8a69c9c02ca09216118c86552704edc23bac179283f2e38f86220ce", size = 5723256, upload-time = "2025-06-16T20:46:27.921Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/d0/00/1e03a4989fa5795da308cd774f05b704ace555a70f9bf9d3be057b680bcf/python_docx-1.2.0-py3-none-any.whl", hash = "sha256:3fd478f3250fbbbfd3b94fe1e985955737c145627498896a8a6bf81f4baf66c7", size = 252987, upload-time = "2025-06-16T20:46:22.506Z" },
+]
+
 [[package]]
 name = "python-dotenv"
 version = "1.2.1"
